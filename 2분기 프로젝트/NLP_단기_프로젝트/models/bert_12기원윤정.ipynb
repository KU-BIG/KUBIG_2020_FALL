{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"bert_12기원윤정.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"80df65ab264d45c8b42e603a0c753dd6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_a724e598b85848429b7c8ec75419fdcb","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_a2ef4c2da3e341c0abe4a556cf0755b0","IPY_MODEL_fc89ba7b63944dd6be4f053fca58164d"]}},"a724e598b85848429b7c8ec75419fdcb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a2ef4c2da3e341c0abe4a556cf0755b0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_c5d63a77f91842e9855603e62ee9f55f","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":995526,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":995526,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_99c5865eee7540d2be4f1677d03e6185"}},"fc89ba7b63944dd6be4f053fca58164d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_5fc7a9651cdd4ba39655ee2f72bb6b94","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 996k/996k [00:01&lt;00:00, 818kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_29d57ce305df49769a493c17d1f70f2a"}},"c5d63a77f91842e9855603e62ee9f55f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"99c5865eee7540d2be4f1677d03e6185":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5fc7a9651cdd4ba39655ee2f72bb6b94":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"29d57ce305df49769a493c17d1f70f2a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"95f166075d7c4ab587481d552a1c2015":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_2823c8c89ab540efa3c901717c61404e","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_218a7ffafb154258980540e5ae9371d2","IPY_MODEL_d071ed5c28bd43b8a0b31faddb1bfeb7"]}},"2823c8c89ab540efa3c901717c61404e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"218a7ffafb154258980540e5ae9371d2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_a75f14d41c644804a9fd612ff0ef81b8","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":625,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":625,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_6147b4974c5e4d7ca254f85a0f654714"}},"d071ed5c28bd43b8a0b31faddb1bfeb7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_7a210892f84f4f9f951f6ff26482a1ce","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 625/625 [00:01&lt;00:00, 538B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f84e6cfaf72c400e9f9eb94bbc5d18b2"}},"a75f14d41c644804a9fd612ff0ef81b8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"6147b4974c5e4d7ca254f85a0f654714":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7a210892f84f4f9f951f6ff26482a1ce":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"f84e6cfaf72c400e9f9eb94bbc5d18b2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"eec25d02c00041e3ad22d05ab161256c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_d6bc98c637cb43b78a71cb1466309919","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_4949e1bfe2db4539bea6d20714670866","IPY_MODEL_096670af27d14ae78a31209ddd6494b0"]}},"d6bc98c637cb43b78a71cb1466309919":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4949e1bfe2db4539bea6d20714670866":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_20da4ca076224b01ba0e1c0084253008","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":714314041,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":714314041,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_7e5bc35a73194b9993686ff3f06f4a6d"}},"096670af27d14ae78a31209ddd6494b0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_42eab019023149679ef0df15b6e94d8e","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 714M/714M [00:53&lt;00:00, 13.4MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e7c10ba2c3534457ae3a039b6c085bdd"}},"20da4ca076224b01ba0e1c0084253008":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"7e5bc35a73194b9993686ff3f06f4a6d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"42eab019023149679ef0df15b6e94d8e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"e7c10ba2c3534457ae3a039b6c085bdd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"id":"WkAHQrj2Vjbl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606297791278,"user_tz":-540,"elapsed":10529,"user":{"displayName":"‍명재성[ 대학원석사과정재학 / 통계학과 ]","photoUrl":"","userId":"11292843267103243221"}},"outputId":"99be2964-a3c5-4cd1-fa32-60b9271798aa"},"source":["# Hugging Face의 트랜스포머 모델을 설치\n","!pip install transformers"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/83/e74092e7f24a08d751aa59b37a9fc572b2e4af3918cb66f7766c3affb1b4/transformers-3.5.1-py3-none-any.whl (1.3MB)\n","\u001b[K     |████████████████████████████████| 1.3MB 4.9MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n","Collecting sentencepiece==0.1.91\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n","\u001b[K     |████████████████████████████████| 1.1MB 15.2MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n","Collecting tokenizers==0.9.3\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/34/b39eb9994bc3c999270b69c9eea40ecc6f0e97991dba28282b9fd32d44ee/tokenizers-0.9.3-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n","\u001b[K     |████████████████████████████████| 2.9MB 21.8MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers) (3.12.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 35.2MB/s \n","\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.11.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (50.3.2)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=a4c4805609c52a6990445ce79fa0179cef115fea00282d2dd6879d2b6f2847c1\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built sacremoses\n","Installing collected packages: sentencepiece, tokenizers, sacremoses, transformers\n","Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.9.3 transformers-3.5.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"75dIz2fNWG8F","executionInfo":{"status":"ok","timestamp":1606297797617,"user_tz":-540,"elapsed":16863,"user":{"displayName":"‍명재성[ 대학원석사과정재학 / 통계학과 ]","photoUrl":"","userId":"11292843267103243221"}}},"source":["import tensorflow as tf\n","import torch\n","\n","from transformers import BertTokenizer\n","from transformers import BertForSequenceClassification, AdamW, BertConfig\n","from transformers import get_linear_schedule_with_warmup\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","from keras.preprocessing.sequence import pad_sequences\n","from sklearn.model_selection import train_test_split\n","\n","import pandas as pd\n","import numpy as np\n","import random\n","import time\n","import datetime\n","import re"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h_U3uMySBCIV"},"source":["<br>\n","<br>\n","\n","# **데이터 로드**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uPx2sDatfNr0","executionInfo":{"status":"ok","timestamp":1606298792056,"user_tz":-540,"elapsed":1011291,"user":{"displayName":"‍명재성[ 대학원석사과정재학 / 통계학과 ]","photoUrl":"","userId":"11292843267103243221"}},"outputId":"15b468e0-6618-44ed-872a-768d19214798"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0LPEdb2tWfIU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606298794495,"user_tz":-540,"elapsed":1013714,"user":{"displayName":"‍명재성[ 대학원석사과정재학 / 통계학과 ]","photoUrl":"","userId":"11292843267103243221"}},"outputId":"17c4bdb7-e930-4cc2-f160-ac68a015afbb"},"source":["# 판다스로 훈련셋과 테스트셋 데이터 로드\n","train = pd.read_csv(\"/content/drive/Shareddrives/20-2 KUBIG 자연어처리 프로젝트/최종 트레이닝데이터/final_training_data.csv\")\n","test = pd.read_csv(\"/content/drive/Shareddrives/20-2 KUBIG 자연어처리 프로젝트/Test_Real_final.csv\")\n","\n","print(train.shape)\n","print(test.shape)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["(99438, 3)\n","(52637, 2)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"V0gps0dYIJgg","executionInfo":{"status":"ok","timestamp":1606298794496,"user_tz":-540,"elapsed":1013712,"user":{"displayName":"‍명재성[ 대학원석사과정재학 / 통계학과 ]","photoUrl":"","userId":"11292843267103243221"}}},"source":["test = test.dropna(how = 'any')"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"tejY9ZhABYWl","colab":{"base_uri":"https://localhost:8080/","height":343},"executionInfo":{"status":"ok","timestamp":1606298794497,"user_tz":-540,"elapsed":1013702,"user":{"displayName":"‍명재성[ 대학원석사과정재학 / 통계학과 ]","photoUrl":"","userId":"11292843267103243221"}},"outputId":"07cd7c04-663c-4e8f-a550-c0c7b20df7b7"},"source":["# 훈련셋의 앞부분 출력\n","train.head(10)"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>document</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>평점에 스포 달린 영화는 무조건 점</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>재밌습니다 잘 만든 영화라는</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>딱 점</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>만남과 헤어짐 그리고 재회</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>롱기스트 야드를 먼저 봐서 그런지 진짜 쓰레기 특히 주인공 진짜 맘에 안 든다</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>5</td>\n","      <td>오늘 고지전을 다시 봤습니다 나라를 위해 휴전 직전까지 싸우다 전사하신 참전용사 여...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>6</td>\n","      <td>술 담배보다 더 해로운 게 막장 드라마다</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>7</td>\n","      <td>정말 좋았는데 특히 영상과 음악</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>8</td>\n","      <td>ㅡㅡ</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>9</td>\n","      <td>예고편에 낚였던 최악의 영화 중 하나</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Unnamed: 0                                           document  label\n","0           0                                평점에 스포 달린 영화는 무조건 점      0\n","1           1                                    재밌습니다 잘 만든 영화라는      1\n","2           2                                                딱 점      0\n","3           3                                     만남과 헤어짐 그리고 재회      1\n","4           4        롱기스트 야드를 먼저 봐서 그런지 진짜 쓰레기 특히 주인공 진짜 맘에 안 든다      0\n","5           5  오늘 고지전을 다시 봤습니다 나라를 위해 휴전 직전까지 싸우다 전사하신 참전용사 여...      1\n","6           6                             술 담배보다 더 해로운 게 막장 드라마다      0\n","7           7                                  정말 좋았는데 특히 영상과 음악      1\n","8           8                                                 ㅡㅡ      0\n","9           9                               예고편에 낚였던 최악의 영화 중 하나      0"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"XgjMzosCDD35"},"source":["<br>\n","<br>\n","\n","# **전처리 - 훈련셋**"]},{"cell_type":"code","metadata":{"id":"2GoESQ0jbybJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606298794497,"user_tz":-540,"elapsed":1013693,"user":{"displayName":"‍명재성[ 대학원석사과정재학 / 통계학과 ]","photoUrl":"","userId":"11292843267103243221"}},"outputId":"13477f52-93b6-4ce3-8373-f547ee75182b"},"source":["# 리뷰 문장 추출\n","sentences = train['document']\n","sentences[:10]"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0                                  평점에 스포 달린 영화는 무조건 점\n","1                                      재밌습니다 잘 만든 영화라는\n","2                                                  딱 점\n","3                                       만남과 헤어짐 그리고 재회\n","4          롱기스트 야드를 먼저 봐서 그런지 진짜 쓰레기 특히 주인공 진짜 맘에 안 든다\n","5    오늘 고지전을 다시 봤습니다 나라를 위해 휴전 직전까지 싸우다 전사하신 참전용사 여...\n","6                               술 담배보다 더 해로운 게 막장 드라마다\n","7                                    정말 좋았는데 특히 영상과 음악\n","8                                                   ㅡㅡ\n","9                                 예고편에 낚였던 최악의 영화 중 하나\n","Name: document, dtype: object"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"8KkJZvhccRUJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606298794498,"user_tz":-540,"elapsed":1013684,"user":{"displayName":"‍명재성[ 대학원석사과정재학 / 통계학과 ]","photoUrl":"","userId":"11292843267103243221"}},"outputId":"e524e901-7f71-400c-821f-fa9a5319575e"},"source":["# BERT의 입력 형식에 맞게 변환\n","sentences = [\"[CLS] \" + str(sentence) + \" [SEP]\" for sentence in sentences]\n","sentences[:10]"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['[CLS] 평점에 스포 달린 영화는 무조건 점 [SEP]',\n"," '[CLS] 재밌습니다 잘 만든 영화라는 [SEP]',\n"," '[CLS] 딱 점 [SEP]',\n"," '[CLS] 만남과 헤어짐 그리고 재회 [SEP]',\n"," '[CLS] 롱기스트 야드를 먼저 봐서 그런지 진짜 쓰레기 특히 주인공 진짜 맘에 안 든다 [SEP]',\n"," '[CLS] 오늘 고지전을 다시 봤습니다 나라를 위해 휴전 직전까지 싸우다 전사하신 참전용사 여러분께 감사드립니다 [SEP]',\n"," '[CLS] 술 담배보다 더 해로운 게 막장 드라마다 [SEP]',\n"," '[CLS] 정말 좋았는데 특히 영상과 음악 [SEP]',\n"," '[CLS] ㅡㅡ [SEP]',\n"," '[CLS] 예고편에 낚였던 최악의 영화 중 하나 [SEP]']"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"7hBblIVQcXJR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606298794499,"user_tz":-540,"elapsed":1013671,"user":{"displayName":"‍명재성[ 대학원석사과정재학 / 통계학과 ]","photoUrl":"","userId":"11292843267103243221"}},"outputId":"3f633788-6425-4d65-ca56-0f652efc581a"},"source":["# 라벨 추출\n","labels = train['label'].values\n","labels"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0, 1, 0, ..., 0, 0, 0])"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"PwEplfDvcnZG","colab":{"base_uri":"https://localhost:8080/","height":102,"referenced_widgets":["80df65ab264d45c8b42e603a0c753dd6","a724e598b85848429b7c8ec75419fdcb","a2ef4c2da3e341c0abe4a556cf0755b0","fc89ba7b63944dd6be4f053fca58164d","c5d63a77f91842e9855603e62ee9f55f","99c5865eee7540d2be4f1677d03e6185","5fc7a9651cdd4ba39655ee2f72bb6b94","29d57ce305df49769a493c17d1f70f2a"]},"executionInfo":{"status":"ok","timestamp":1606298816327,"user_tz":-540,"elapsed":1035485,"user":{"displayName":"‍명재성[ 대학원석사과정재학 / 통계학과 ]","photoUrl":"","userId":"11292843267103243221"}},"outputId":"b8d7d47f-650e-47e5-fefd-f33442265fcb"},"source":["# BERT의 토크나이저로 문장을 토큰으로 분리\n","tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=False)\n","tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n","\n","print (sentences[0])\n","print (tokenized_texts[0])"],"execution_count":10,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"80df65ab264d45c8b42e603a0c753dd6","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=995526.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","[CLS] 평점에 스포 달린 영화는 무조건 점 [SEP]\n","['[CLS]', '평', '##점', '##에', '스', '##포', '달', '##린', '영화', '##는', '무', '##조', '##건', '점', '[SEP]']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"VJ76KiP_dLn-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606298820028,"user_tz":-540,"elapsed":1039175,"user":{"displayName":"‍명재성[ 대학원석사과정재학 / 통계학과 ]","photoUrl":"","userId":"11292843267103243221"}},"outputId":"228469c7-a381-414d-d1ba-6cffdab2faf3"},"source":["# 입력 토큰의 최대 시퀀스 길이\n","MAX_LEN = 128\n","\n","# 토큰을 숫자 인덱스로 변환\n","input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n","\n","# 문장을 MAX_LEN 길이에 맞게 자르고, 모자란 부분을 패딩 0으로 채움\n","input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n","\n","input_ids[0]"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([  101,  9926, 34907, 10530,  9477, 55530,  9061, 27654, 42428,\n","       11018,  9294, 20626, 71439,  9668,   102,     0,     0,     0,\n","           0,     0,     0,     0,     0,     0,     0,     0,     0,\n","           0,     0,     0,     0,     0,     0,     0,     0,     0,\n","           0,     0,     0,     0,     0,     0,     0,     0,     0,\n","           0,     0,     0,     0,     0,     0,     0,     0,     0,\n","           0,     0,     0,     0,     0,     0,     0,     0,     0,\n","           0,     0,     0,     0,     0,     0,     0,     0,     0,\n","           0,     0,     0,     0,     0,     0,     0,     0,     0,\n","           0,     0,     0,     0,     0,     0,     0,     0,     0,\n","           0,     0,     0,     0,     0,     0,     0,     0,     0,\n","           0,     0,     0,     0,     0,     0,     0,     0,     0,\n","           0,     0,     0,     0,     0,     0,     0,     0,     0,\n","           0,     0,     0,     0,     0,     0,     0,     0,     0,\n","           0,     0])"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"pKfL8SotdVaW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606298829173,"user_tz":-540,"elapsed":1048308,"user":{"displayName":"‍명재성[ 대학원석사과정재학 / 통계학과 ]","photoUrl":"","userId":"11292843267103243221"}},"outputId":"6e8f02e3-c4e2-4d44-b40a-9aaf7a0e5150"},"source":["# 어텐션 마스크 초기화\n","attention_masks = []\n","\n","# 어텐션 마스크를 패딩이 아니면 1, 패딩이면 0으로 설정\n","# 패딩 부분은 BERT 모델에서 어텐션을 수행하지 않아 속도 향상\n","for seq in input_ids:\n","    seq_mask = [float(i>0) for i in seq]\n","    attention_masks.append(seq_mask)\n","\n","print(attention_masks[0])"],"execution_count":12,"outputs":[{"output_type":"stream","text":["[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1f5Vq3-7eNKH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606298830644,"user_tz":-540,"elapsed":1049766,"user":{"displayName":"‍명재성[ 대학원석사과정재학 / 통계학과 ]","photoUrl":"","userId":"11292843267103243221"}},"outputId":"ad44eec7-c4fc-41b1-928a-ca4e9f2d4db0"},"source":["# 훈련셋과 검증셋으로 분리\n","train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids,\n","                                                                                    labels, \n","                                                                                    random_state=2018, \n","                                                                                    test_size=0.1)\n","\n","# 어텐션 마스크를 훈련셋과 검증셋으로 분리\n","train_masks, validation_masks, _, _ = train_test_split(attention_masks, \n","                                                       input_ids,\n","                                                       random_state=2018, \n","                                                       test_size=0.1)\n","\n","# 데이터를 파이토치의 텐서로 변환\n","train_inputs = torch.tensor(train_inputs)\n","train_labels = torch.tensor(train_labels)\n","train_masks = torch.tensor(train_masks)\n","validation_inputs = torch.tensor(validation_inputs)\n","validation_labels = torch.tensor(validation_labels)\n","validation_masks = torch.tensor(validation_masks)\t\t\t\t\n","\n","print(train_inputs[0])\n","print(train_labels[0])\n","print(train_masks[0])\n","print(validation_inputs[0])\n","print(validation_labels[0])\n","print(validation_masks[0])"],"execution_count":13,"outputs":[{"output_type":"stream","text":["tensor([   101,   9781,  17196,  26212,   9638,  42428,  11513,   9765,  38631,\n","         36251, 100585,   8982, 119023,   9016,  40958,  10739,  17737,    102,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0])\n","tensor(0)\n","tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0.])\n","tensor([  101,  9749, 30005, 30858, 18227, 10459,  9318, 10739, 92564,   102,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0])\n","tensor(1)\n","tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0.])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"I3vlyUJuVRo5","executionInfo":{"status":"ok","timestamp":1606298830645,"user_tz":-540,"elapsed":1049764,"user":{"displayName":"‍명재성[ 대학원석사과정재학 / 통계학과 ]","photoUrl":"","userId":"11292843267103243221"}}},"source":["# 배치 사이즈\n","batch_size = 32\n","\n","# 파이토치의 DataLoader로 입력, 마스크, 라벨을 묶어 데이터 설정\n","# 학습시 배치 사이즈 만큼 데이터를 가져옴\n","train_data = TensorDataset(train_inputs, train_masks, train_labels)\n","train_sampler = RandomSampler(train_data)\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","\n","validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n","validation_sampler = SequentialSampler(validation_data)\n","validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zkqUHx51dffp"},"source":["<br>\n","<br>\n","\n","# **전처리 - 테스트셋**"]},{"cell_type":"code","metadata":{"id":"xgrsNuArd4pj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606298830645,"user_tz":-540,"elapsed":1049756,"user":{"displayName":"‍명재성[ 대학원석사과정재학 / 통계학과 ]","photoUrl":"","userId":"11292843267103243221"}},"outputId":"38d8ec12-bb63-44ab-9e54-187556f51d43"},"source":["# 리뷰 문장 추출\n","sentences = test['0']\n","sentences[:10]"],"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0                                                   굳 ㅋ\n","2                     뭐 야 이 평점들은 나쁘진 않지만 점 짜리는 더더욱 아니잖아\n","3                           지루하지는 않은데 완전 막장 임 돈 주고 보기에는\n","4            만 아니었어도 별 다섯 개 줬을 텐데 왜 로 나와서 제 심기를 불편하게 하죠\n","5                                     음악이 주가 된 최고의 음악영화\n","6                                               진정한 쓰레기\n","7        마치 미국 애니에서 튀어나온 듯한 창의력 없는 로봇 디자인부터 가 고개를 젖게 한다\n","8     갈수록 개판되가는 중국 영화 유치하고 내용 없음 폼 잡다 끝남 말도 안 되는 무기에...\n","9             이별의 아픔 뒤에 찾아오는 새로운 인연의 기 쁨 모든 사람이 그렇지는 않네\n","10                                  괜찮네요 오랜만 포켓몬스터 잼밌어요\n","Name: 0, dtype: object"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"id":"Gtz3QZt9d4pz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606298830646,"user_tz":-540,"elapsed":1049747,"user":{"displayName":"‍명재성[ 대학원석사과정재학 / 통계학과 ]","photoUrl":"","userId":"11292843267103243221"}},"outputId":"ee8adb5c-4073-49be-9c14-11bf6630bcf7"},"source":["# BERT의 입력 형식에 맞게 변환\n","sentences = [\"[CLS] \" + str(sentence) + \" [SEP]\" for sentence in sentences]\n","sentences[:10]"],"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['[CLS] 굳 ㅋ [SEP]',\n"," '[CLS] 뭐 야 이 평점들은 나쁘진 않지만 점 짜리는 더더욱 아니잖아 [SEP]',\n"," '[CLS] 지루하지는 않은데 완전 막장 임 돈 주고 보기에는 [SEP]',\n"," '[CLS] 만 아니었어도 별 다섯 개 줬을 텐데 왜 로 나와서 제 심기를 불편하게 하죠 [SEP]',\n"," '[CLS] 음악이 주가 된 최고의 음악영화 [SEP]',\n"," '[CLS] 진정한 쓰레기 [SEP]',\n"," '[CLS] 마치 미국 애니에서 튀어나온 듯한 창의력 없는 로봇 디자인부터 가 고개를 젖게 한다 [SEP]',\n"," '[CLS] 갈수록 개판되가는 중국 영화 유치하고 내용 없음 폼 잡다 끝남 말도 안 되는 무기에 유치한 남무 아 그립다 동사서독 같은 영화가 이건 류아류작이다 [SEP]',\n"," '[CLS] 이별의 아픔 뒤에 찾아오는 새로운 인연의 기 쁨 모든 사람이 그렇지는 않네 [SEP]',\n"," '[CLS] 괜찮네요 오랜만 포켓몬스터 잼밌어요 [SEP]']"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"li8oRajbd4p3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606298830646,"user_tz":-540,"elapsed":1049739,"user":{"displayName":"‍명재성[ 대학원석사과정재학 / 통계학과 ]","photoUrl":"","userId":"11292843267103243221"}},"outputId":"d938bf68-a48b-4dea-a5f4-732a5ae751d5"},"source":["# 라벨 추출\n","labels = test['label'].values\n","labels"],"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1, 0, 0, ..., 0, 1, 1])"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"code","metadata":{"id":"lvpQ49nEd4p6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606298842463,"user_tz":-540,"elapsed":1061548,"user":{"displayName":"‍명재성[ 대학원석사과정재학 / 통계학과 ]","photoUrl":"","userId":"11292843267103243221"}},"outputId":"329a3f54-6cf8-4deb-c123-1a949f3a55a6"},"source":["# BERT의 토크나이저로 문장을 토큰으로 분리\n","tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=False)\n","tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n","\n","print (sentences[0])\n","print (tokenized_texts[0])"],"execution_count":18,"outputs":[{"output_type":"stream","text":["[CLS] 굳 ㅋ [SEP]\n","['[CLS]', '굳', '[UNK]', '[SEP]']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"HI9viuAvd4p_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606298843954,"user_tz":-540,"elapsed":1063031,"user":{"displayName":"‍명재성[ 대학원석사과정재학 / 통계학과 ]","photoUrl":"","userId":"11292843267103243221"}},"outputId":"676bc35a-df41-4984-f989-4e15541b03d1"},"source":["# 입력 토큰의 최대 시퀀스 길이\n","MAX_LEN = 128\n","\n","# 토큰을 숫자 인덱스로 변환\n","input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n","\n","# 문장을 MAX_LEN 길이에 맞게 자르고, 모자란 부분을 패딩 0으로 채움\n","input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n","\n","input_ids[0]"],"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 101, 8911,  100,  102,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0])"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"code","metadata":{"id":"v1NKmP0Fd4qD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606298848767,"user_tz":-540,"elapsed":1067836,"user":{"displayName":"‍명재성[ 대학원석사과정재학 / 통계학과 ]","photoUrl":"","userId":"11292843267103243221"}},"outputId":"af753a02-33f3-4036-b016-9e3bb8fb1cd9"},"source":["# 어텐션 마스크 초기화\n","attention_masks = []\n","\n","# 어텐션 마스크를 패딩이 아니면 1, 패딩이면 0으로 설정\n","# 패딩 부분은 BERT 모델에서 어텐션을 수행하지 않아 속도 향상\n","for seq in input_ids:\n","    seq_mask = [float(i>0) for i in seq]\n","    attention_masks.append(seq_mask)\n","\n","print(attention_masks[0])"],"execution_count":20,"outputs":[{"output_type":"stream","text":["[1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RIkaYCGbd4qG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606298849217,"user_tz":-540,"elapsed":1068278,"user":{"displayName":"‍명재성[ 대학원석사과정재학 / 통계학과 ]","photoUrl":"","userId":"11292843267103243221"}},"outputId":"99503489-84f0-4d5e-f2f7-c4abcb13a74e"},"source":["# 데이터를 파이토치의 텐서로 변환\n","test_inputs = torch.tensor(input_ids)\n","test_labels = torch.tensor(labels)\n","test_masks = torch.tensor(attention_masks)\n","\n","print(test_inputs[0])\n","print(test_labels[0])\n","print(test_masks[0])"],"execution_count":21,"outputs":[{"output_type":"stream","text":["tensor([ 101, 8911,  100,  102,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0])\n","tensor(1)\n","tensor([1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0.])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7gwdYv1Ad4qK","executionInfo":{"status":"ok","timestamp":1606298849218,"user_tz":-540,"elapsed":1068277,"user":{"displayName":"‍명재성[ 대학원석사과정재학 / 통계학과 ]","photoUrl":"","userId":"11292843267103243221"}}},"source":["# 배치 사이즈\n","batch_size = 32\n","\n","# 파이토치의 DataLoader로 입력, 마스크, 라벨을 묶어 데이터 설정\n","# 학습시 배치 사이즈 만큼 데이터를 가져옴\n","test_data = TensorDataset(test_inputs, test_masks, test_labels)\n","test_sampler = RandomSampler(test_data)\n","test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"],"execution_count":22,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FBvpU-Hfgcth"},"source":["<br>\n","<br>\n","\n","# **모델 생성**"]},{"cell_type":"code","metadata":{"id":"heToD1ev0mOg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606298855655,"user_tz":-540,"elapsed":1074706,"user":{"displayName":"‍명재성[ 대학원석사과정재학 / 통계학과 ]","photoUrl":"","userId":"11292843267103243221"}},"outputId":"fa294c83-4d3c-4346-d846-b5a431bdf66b"},"source":["# GPU 디바이스 이름 구함\n","device_name = tf.test.gpu_device_name()\n","\n","# GPU 디바이스 이름 검사\n","if device_name == '/device:GPU:0':\n","    print('Found GPU at: {}'.format(device_name))\n","else:\n","    raise SystemError('GPU device not found')"],"execution_count":23,"outputs":[{"output_type":"stream","text":["Found GPU at: /device:GPU:0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"f6enIxvt1FB2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606298855656,"user_tz":-540,"elapsed":1074699,"user":{"displayName":"‍명재성[ 대학원석사과정재학 / 통계학과 ]","photoUrl":"","userId":"11292843267103243221"}},"outputId":"c28e9e6d-bdbd-4a6d-fc97-b37d64aefadf"},"source":["# 디바이스 설정\n","if torch.cuda.is_available():    \n","    device = torch.device(\"cuda\")\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","else:\n","    device = torch.device(\"cpu\")\n","    print('No GPU available, using the CPU instead.')"],"execution_count":24,"outputs":[{"output_type":"stream","text":["There are 1 GPU(s) available.\n","We will use the GPU: Tesla K80\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MS2MXSiLg5zC","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["95f166075d7c4ab587481d552a1c2015","2823c8c89ab540efa3c901717c61404e","218a7ffafb154258980540e5ae9371d2","d071ed5c28bd43b8a0b31faddb1bfeb7","a75f14d41c644804a9fd612ff0ef81b8","6147b4974c5e4d7ca254f85a0f654714","7a210892f84f4f9f951f6ff26482a1ce","f84e6cfaf72c400e9f9eb94bbc5d18b2","eec25d02c00041e3ad22d05ab161256c","d6bc98c637cb43b78a71cb1466309919","4949e1bfe2db4539bea6d20714670866","096670af27d14ae78a31209ddd6494b0","20da4ca076224b01ba0e1c0084253008","7e5bc35a73194b9993686ff3f06f4a6d","42eab019023149679ef0df15b6e94d8e","e7c10ba2c3534457ae3a039b6c085bdd"]},"executionInfo":{"status":"ok","timestamp":1606298898317,"user_tz":-540,"elapsed":1117351,"user":{"displayName":"‍명재성[ 대학원석사과정재학 / 통계학과 ]","photoUrl":"","userId":"11292843267103243221"}},"outputId":"6ce673e7-6fe7-4295-d10b-c2be70b8e81c"},"source":["# 분류를 위한 BERT 모델 생성\n","model = BertForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels=2)\n","model.cuda()"],"execution_count":25,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"95f166075d7c4ab587481d552a1c2015","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=625.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"eec25d02c00041e3ad22d05ab161256c","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=714314041.0, style=ProgressStyle(descri…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["BertForSequenceClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"code","metadata":{"id":"ZIdfbLTuWmxk","executionInfo":{"status":"ok","timestamp":1606298898318,"user_tz":-540,"elapsed":1117348,"user":{"displayName":"‍명재성[ 대학원석사과정재학 / 통계학과 ]","photoUrl":"","userId":"11292843267103243221"}}},"source":["# 옵티마이저 설정\n","optimizer = AdamW(model.parameters(),\n","                  lr = 2e-5, # 학습률\n","                  eps = 1e-8 # 0으로 나누는 것을 방지하기 위한 epsilon 값\n","                )\n","\n","# 에폭수\n","epochs = 3\n","\n","# 총 훈련 스텝 : 배치반복 횟수 * 에폭\n","total_steps = len(train_dataloader) * epochs\n","\n","# 처음에 학습률을 조금씩 변화시키는 스케줄러 생성\n","scheduler = get_linear_schedule_with_warmup(optimizer, \n","                                            num_warmup_steps = 0,\n","                                            num_training_steps = total_steps)"],"execution_count":26,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gzCHV_ghj7DM"},"source":["<br>\n","<br>\n","\n","# **모델 학습**"]},{"cell_type":"code","metadata":{"id":"S0-p6pPVXCRe","executionInfo":{"status":"ok","timestamp":1606298898319,"user_tz":-540,"elapsed":1117347,"user":{"displayName":"‍명재성[ 대학원석사과정재학 / 통계학과 ]","photoUrl":"","userId":"11292843267103243221"}}},"source":["# 정확도 계산 함수\n","def flat_accuracy(preds, labels):\n","    \n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)"],"execution_count":27,"outputs":[]},{"cell_type":"code","metadata":{"id":"FJXISnJzCdLM","executionInfo":{"status":"ok","timestamp":1606298898319,"user_tz":-540,"elapsed":1117345,"user":{"displayName":"‍명재성[ 대학원석사과정재학 / 통계학과 ]","photoUrl":"","userId":"11292843267103243221"}}},"source":["# 시간 표시 함수\n","def format_time(elapsed):\n","\n","    # 반올림\n","    elapsed_rounded = int(round((elapsed)))\n","    \n","    # hh:mm:ss으로 형태 변경\n","    return str(datetime.timedelta(seconds=elapsed_rounded))"],"execution_count":28,"outputs":[]},{"cell_type":"code","metadata":{"id":"muU2kS2GCh4y","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606309866765,"user_tz":-540,"elapsed":12085783,"user":{"displayName":"‍명재성[ 대학원석사과정재학 / 통계학과 ]","photoUrl":"","userId":"11292843267103243221"}},"outputId":"fe66776a-9656-4b6a-f0e0-0368ca48a4e1"},"source":["# 재현을 위해 랜덤시드 고정\n","seed_val = 42\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","\n","# 그래디언트 초기화\n","model.zero_grad()\n","\n","# 에폭만큼 반복\n","for epoch_i in range(0, epochs):\n","    \n","    # ========================================\n","    #               Training\n","    # ========================================\n","    \n","    print(\"\")\n","    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","    print('Training...')\n","\n","    # 시작 시간 설정\n","    t0 = time.time()\n","\n","    # 로스 초기화\n","    total_loss = 0\n","\n","    # 훈련모드로 변경\n","    model.train()\n","        \n","    # 데이터로더에서 배치만큼 반복하여 가져옴\n","    for step, batch in enumerate(train_dataloader):\n","        # 경과 정보 표시\n","        if step % 500 == 0 and not step == 0:\n","            elapsed = format_time(time.time() - t0)\n","            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n","\n","        # 배치를 GPU에 넣음\n","        batch = tuple(t.to(device) for t in batch)\n","        \n","        # 배치에서 데이터 추출\n","        b_input_ids, b_input_mask, b_labels = batch\n","\n","        # Forward 수행                \n","        outputs = model(b_input_ids, \n","                        token_type_ids=None, \n","                        attention_mask=b_input_mask, \n","                        labels=b_labels)\n","        \n","        # 로스 구함\n","        loss = outputs[0]\n","\n","        # 총 로스 계산\n","        total_loss += loss.item()\n","\n","        # Backward 수행으로 그래디언트 계산\n","        loss.backward()\n","\n","        # 그래디언트 클리핑\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","        # 그래디언트를 통해 가중치 파라미터 업데이트\n","        optimizer.step()\n","\n","        # 스케줄러로 학습률 감소\n","        scheduler.step()\n","\n","        # 그래디언트 초기화\n","        model.zero_grad()\n","\n","    # 평균 로스 계산\n","    avg_train_loss = total_loss / len(train_dataloader)            \n","\n","    print(\"\")\n","    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n","        \n","    # ========================================\n","    #               Validation\n","    # ========================================\n","\n","    print(\"\")\n","    print(\"Running Validation...\")\n","\n","    #시작 시간 설정\n","    t0 = time.time()\n","\n","    # 평가모드로 변경\n","    model.eval()\n","\n","    # 변수 초기화\n","    eval_loss, eval_accuracy = 0, 0\n","    nb_eval_steps, nb_eval_examples = 0, 0\n","\n","    # 데이터로더에서 배치만큼 반복하여 가져옴\n","    for batch in validation_dataloader:\n","        # 배치를 GPU에 넣음\n","        batch = tuple(t.to(device) for t in batch)\n","        \n","        # 배치에서 데이터 추출\n","        b_input_ids, b_input_mask, b_labels = batch\n","        \n","        # 그래디언트 계산 안함\n","        with torch.no_grad():     \n","            # Forward 수행\n","            outputs = model(b_input_ids, \n","                            token_type_ids=None, \n","                            attention_mask=b_input_mask)\n","        \n","        # 로스 구함\n","        logits = outputs[0]\n","\n","        # CPU로 데이터 이동\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","        \n","        # 출력 로짓과 라벨을 비교하여 정확도 계산\n","        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n","        eval_accuracy += tmp_eval_accuracy\n","        nb_eval_steps += 1\n","\n","    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n","    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n","\n","print(\"\")\n","print(\"Training complete!\")"],"execution_count":29,"outputs":[{"output_type":"stream","text":["\n","======== Epoch 1 / 3 ========\n","Training...\n","  Batch   500  of  2,797.    Elapsed: 0:10:25.\n","  Batch 1,000  of  2,797.    Elapsed: 0:20:51.\n","  Batch 1,500  of  2,797.    Elapsed: 0:31:17.\n","  Batch 2,000  of  2,797.    Elapsed: 0:41:43.\n","  Batch 2,500  of  2,797.    Elapsed: 0:52:11.\n","\n","  Average training loss: 0.41\n","  Training epcoh took: 0:58:28\n","\n","Running Validation...\n","  Accuracy: 0.85\n","  Validation took: 0:02:17\n","\n","======== Epoch 2 / 3 ========\n","Training...\n","  Batch   500  of  2,797.    Elapsed: 0:10:36.\n","  Batch 1,000  of  2,797.    Elapsed: 0:21:13.\n","  Batch 1,500  of  2,797.    Elapsed: 0:31:50.\n","  Batch 2,000  of  2,797.    Elapsed: 0:42:18.\n","  Batch 2,500  of  2,797.    Elapsed: 0:52:44.\n","\n","  Average training loss: 0.30\n","  Training epcoh took: 0:58:56\n","\n","Running Validation...\n","  Accuracy: 0.86\n","  Validation took: 0:02:17\n","\n","======== Epoch 3 / 3 ========\n","Training...\n","  Batch   500  of  2,797.    Elapsed: 0:10:36.\n","  Batch 1,000  of  2,797.    Elapsed: 0:21:12.\n","  Batch 1,500  of  2,797.    Elapsed: 0:31:38.\n","  Batch 2,000  of  2,797.    Elapsed: 0:42:03.\n","  Batch 2,500  of  2,797.    Elapsed: 0:52:26.\n","\n","  Average training loss: 0.24\n","  Training epcoh took: 0:58:36\n","\n","Running Validation...\n","  Accuracy: 0.87\n","  Validation took: 0:02:14\n","\n","Training complete!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"6BVbl4Zjatzn"},"source":["<br>\n","<br>\n","\n","# **테스트셋 평가**"]},{"cell_type":"code","metadata":{"id":"c5KHb6RkbHdj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606310569365,"user_tz":-540,"elapsed":12788375,"user":{"displayName":"‍명재성[ 대학원석사과정재학 / 통계학과 ]","photoUrl":"","userId":"11292843267103243221"}},"outputId":"6170cfee-aa6c-46de-80a0-4dc589dc2cfd"},"source":["#시작 시간 설정\n","t0 = time.time()\n","\n","# 평가모드로 변경\n","model.eval()\n","\n","# 변수 초기화\n","eval_loss, eval_accuracy = 0, 0\n","nb_eval_steps, nb_eval_examples = 0, 0\n","\n","# 데이터로더에서 배치만큼 반복하여 가져옴\n","for step, batch in enumerate(test_dataloader):\n","    # 경과 정보 표시\n","    if step % 100 == 0 and not step == 0:\n","        elapsed = format_time(time.time() - t0)\n","        print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(test_dataloader), elapsed))\n","\n","    # 배치를 GPU에 넣음\n","    batch = tuple(t.to(device) for t in batch)\n","    \n","    # 배치에서 데이터 추출\n","    b_input_ids, b_input_mask, b_labels = batch\n","    \n","    # 그래디언트 계산 안함\n","    with torch.no_grad():     \n","        # Forward 수행\n","        outputs = model(b_input_ids, \n","                        token_type_ids=None, \n","                        attention_mask=b_input_mask)\n","    \n","    # 로스 구함\n","    logits = outputs[0]\n","\n","    # CPU로 데이터 이동\n","    logits = logits.detach().cpu().numpy()\n","    label_ids = b_labels.to('cpu').numpy()\n","    \n","    # 출력 로짓과 라벨을 비교하여 정확도 계산\n","    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n","    eval_accuracy += tmp_eval_accuracy\n","    nb_eval_steps += 1\n","\n","print(\"\")\n","print(\"Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n","print(\"Test took: {:}\".format(format_time(time.time() - t0)))"],"execution_count":30,"outputs":[{"output_type":"stream","text":["  Batch   100  of  1,633.    Elapsed: 0:00:43.\n","  Batch   200  of  1,633.    Elapsed: 0:01:26.\n","  Batch   300  of  1,633.    Elapsed: 0:02:09.\n","  Batch   400  of  1,633.    Elapsed: 0:02:52.\n","  Batch   500  of  1,633.    Elapsed: 0:03:35.\n","  Batch   600  of  1,633.    Elapsed: 0:04:18.\n","  Batch   700  of  1,633.    Elapsed: 0:05:01.\n","  Batch   800  of  1,633.    Elapsed: 0:05:44.\n","  Batch   900  of  1,633.    Elapsed: 0:06:27.\n","  Batch 1,000  of  1,633.    Elapsed: 0:07:10.\n","  Batch 1,100  of  1,633.    Elapsed: 0:07:53.\n","  Batch 1,200  of  1,633.    Elapsed: 0:08:36.\n","  Batch 1,300  of  1,633.    Elapsed: 0:09:19.\n","  Batch 1,400  of  1,633.    Elapsed: 0:10:02.\n","  Batch 1,500  of  1,633.    Elapsed: 0:10:45.\n","  Batch 1,600  of  1,633.    Elapsed: 0:11:28.\n","\n","Accuracy: 0.85\n","Test took: 0:11:42\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"iy3PSYD9Hxy7"},"source":["85%의 정확도"]},{"cell_type":"markdown","metadata":{"id":"U7SzL1IBe1Dm"},"source":["<br>\n","<br>\n","\n","# **새로운 문장 테스트**"]},{"cell_type":"code","metadata":{"id":"Tb4v_VfEfGQB","executionInfo":{"status":"ok","timestamp":1606310569367,"user_tz":-540,"elapsed":12788376,"user":{"displayName":"‍명재성[ 대학원석사과정재학 / 통계학과 ]","photoUrl":"","userId":"11292843267103243221"}}},"source":["# 입력 데이터 변환\n","def convert_input_data(sentences):\n","\n","    # BERT의 토크나이저로 문장을 토큰으로 분리\n","    tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n","\n","    # 입력 토큰의 최대 시퀀스 길이\n","    MAX_LEN = 128\n","\n","    # 토큰을 숫자 인덱스로 변환\n","    input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n","    \n","    # 문장을 MAX_LEN 길이에 맞게 자르고, 모자란 부분을 패딩 0으로 채움\n","    input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n","\n","    # 어텐션 마스크 초기화\n","    attention_masks = []\n","\n","    # 어텐션 마스크를 패딩이 아니면 1, 패딩이면 0으로 설정\n","    # 패딩 부분은 BERT 모델에서 어텐션을 수행하지 않아 속도 향상\n","    for seq in input_ids:\n","        seq_mask = [float(i>0) for i in seq]\n","        attention_masks.append(seq_mask)\n","\n","    # 데이터를 파이토치의 텐서로 변환\n","    inputs = torch.tensor(input_ids)\n","    masks = torch.tensor(attention_masks)\n","\n","    return inputs, masks"],"execution_count":31,"outputs":[]},{"cell_type":"code","metadata":{"id":"C12NL1Fvgv4E","executionInfo":{"status":"ok","timestamp":1606310569368,"user_tz":-540,"elapsed":12788375,"user":{"displayName":"‍명재성[ 대학원석사과정재학 / 통계학과 ]","photoUrl":"","userId":"11292843267103243221"}}},"source":["# 문장 테스트\n","def test_sentences(sentences):\n","\n","    # 평가모드로 변경\n","    model.eval()\n","\n","    # 문장을 입력 데이터로 변환\n","    inputs, masks = convert_input_data(sentences)\n","\n","    # 데이터를 GPU에 넣음\n","    b_input_ids = inputs.to(device)\n","    b_input_mask = masks.to(device)\n","            \n","    # 그래디언트 계산 안함\n","    with torch.no_grad():     \n","        # Forward 수행\n","        outputs = model(b_input_ids, \n","                        token_type_ids=None, \n","                        attention_mask=b_input_mask)\n","\n","    # 로스 구함\n","    logits = outputs[0]\n","\n","    # CPU로 데이터 이동\n","    logits = logits.detach().cpu().numpy()\n","\n","    return logits"],"execution_count":32,"outputs":[]},{"cell_type":"code","metadata":{"id":"J700-Yd9g8TJ","executionInfo":{"status":"ok","timestamp":1606311235575,"user_tz":-540,"elapsed":974,"user":{"displayName":"‍명재성[ 대학원석사과정재학 / 통계학과 ]","photoUrl":"","userId":"11292843267103243221"}}},"source":["def prob(logits):\n","  logits = logits\n","  if(logits[0][0] > logits[0][1]):\n","    print(\"약 {:.2f}% 확률로 부정 리뷰입니다.\".format(100 / (1+ np.exp(-logits[0][0]))))\n","  else:\n","    print(\"약 {:.2f}% 확률로 긍정 리뷰입니다.\".format(100 / (1+ np.exp(-logits[0][1]))))"],"execution_count":60,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZQezr0tljJlM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606311236945,"user_tz":-540,"elapsed":825,"user":{"displayName":"‍명재성[ 대학원석사과정재학 / 통계학과 ]","photoUrl":"","userId":"11292843267103243221"}},"outputId":"3a4fa7da-fa2e-4bee-bb38-140f2e4b1069"},"source":["logits = test_sentences(['연기는 별로지만 재미 하나는 끝내줌!'])\n","prob(logits)"],"execution_count":61,"outputs":[{"output_type":"stream","text":["약 78.16% 확률로 긍정 리뷰입니다.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-9MQ0SK0jofN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606311246526,"user_tz":-540,"elapsed":889,"user":{"displayName":"‍명재성[ 대학원석사과정재학 / 통계학과 ]","photoUrl":"","userId":"11292843267103243221"}},"outputId":"6d954170-7d35-4a51-a2b5-b9be146fcbb4"},"source":["logits = test_sentences(['주연배우가 아깝다. 총체적 난국...'])\n","prob(logits)"],"execution_count":62,"outputs":[{"output_type":"stream","text":["약 95.66% 확률로 부정 리뷰입니다.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bAHEw_UYlTCs","executionInfo":{"status":"ok","timestamp":1606311276343,"user_tz":-540,"elapsed":863,"user":{"displayName":"‍명재성[ 대학원석사과정재학 / 통계학과 ]","photoUrl":"","userId":"11292843267103243221"}},"outputId":"5f6e295a-880d-4193-c4c5-47b5eae5717f"},"source":["logits = test_sentences(['노답이네 이게 뭐냐'])\n","prob(logits)"],"execution_count":63,"outputs":[{"output_type":"stream","text":["약 87.39% 확률로 부정 리뷰입니다.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-zXkullpnPru","executionInfo":{"status":"ok","timestamp":1606311290129,"user_tz":-540,"elapsed":1011,"user":{"displayName":"‍명재성[ 대학원석사과정재학 / 통계학과 ]","photoUrl":"","userId":"11292843267103243221"}},"outputId":"d62fae94-4240-42a4-ea80-74ab5e2ed48a"},"source":["logits = test_sentences(['최악이다 영상 왜이래'])\n","prob(logits)"],"execution_count":64,"outputs":[{"output_type":"stream","text":["약 95.64% 확률로 부정 리뷰입니다.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lUQxWNxgnS_V","executionInfo":{"status":"ok","timestamp":1606311300566,"user_tz":-540,"elapsed":843,"user":{"displayName":"‍명재성[ 대학원석사과정재학 / 통계학과 ]","photoUrl":"","userId":"11292843267103243221"}},"outputId":"c8fb629c-5ed8-4142-af99-49859d55195e"},"source":["logits = test_sentences(['반하겠다 잘생김 한도 초과'])\n","prob(logits)"],"execution_count":65,"outputs":[{"output_type":"stream","text":["약 90.49% 확률로 부정 리뷰입니다.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RC-1N1tlnXB0","executionInfo":{"status":"ok","timestamp":1606311316412,"user_tz":-540,"elapsed":1032,"user":{"displayName":"‍명재성[ 대학원석사과정재학 / 통계학과 ]","photoUrl":"","userId":"11292843267103243221"}},"outputId":"ca0b5bd5-d4c7-4fb9-db97-126b5b8cbead"},"source":["logits = test_sentences(['색감이 대박'])\n","prob(logits)"],"execution_count":66,"outputs":[{"output_type":"stream","text":["약 82.47% 확률로 긍정 리뷰입니다.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p3KuZ5P6nZcs","executionInfo":{"status":"ok","timestamp":1606311327437,"user_tz":-540,"elapsed":841,"user":{"displayName":"‍명재성[ 대학원석사과정재학 / 통계학과 ]","photoUrl":"","userId":"11292843267103243221"}},"outputId":"37cf811d-8241-490d-a2f2-ea295c238ee3"},"source":["logits = test_sentences(['무조건 2탄 나와야된다'])\n","prob(logits)"],"execution_count":67,"outputs":[{"output_type":"stream","text":["약 57.03% 확률로 부정 리뷰입니다.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9naQWeOgnccx","executionInfo":{"status":"ok","timestamp":1606311348983,"user_tz":-540,"elapsed":843,"user":{"displayName":"‍명재성[ 대학원석사과정재학 / 통계학과 ]","photoUrl":"","userId":"11292843267103243221"}},"outputId":"db4c5a48-0f3e-4097-fb0b-cb9ed436696d"},"source":["logits = test_sentences(['남편이 빌런이네 아내 불쌍해'])\n","prob(logits)"],"execution_count":68,"outputs":[{"output_type":"stream","text":["약 51.44% 확률로 부정 리뷰입니다.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IXUT2Ph2nhCU","executionInfo":{"status":"ok","timestamp":1606311357760,"user_tz":-540,"elapsed":875,"user":{"displayName":"‍명재성[ 대학원석사과정재학 / 통계학과 ]","photoUrl":"","userId":"11292843267103243221"}},"outputId":"0da0e167-b50c-4645-f9fa-14bedb715a03"},"source":["logits = test_sentences(['엄마랑 보다가 사레 걸리는줄'])\n","prob(logits)"],"execution_count":69,"outputs":[{"output_type":"stream","text":["약 80.10% 확률로 부정 리뷰입니다.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tjfD_054njfs","executionInfo":{"status":"ok","timestamp":1606311366110,"user_tz":-540,"elapsed":1013,"user":{"displayName":"‍명재성[ 대학원석사과정재학 / 통계학과 ]","photoUrl":"","userId":"11292843267103243221"}},"outputId":"d8e34431-46d8-4262-b74f-be2312cb8b0b"},"source":["logits = test_sentences(['평생 구독할게요 감사합니다'])\n","prob(logits)"],"execution_count":70,"outputs":[{"output_type":"stream","text":["약 90.07% 확률로 긍정 리뷰입니다.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CZbMThJ2nlJ3","executionInfo":{"status":"ok","timestamp":1606311375075,"user_tz":-540,"elapsed":868,"user":{"displayName":"‍명재성[ 대학원석사과정재학 / 통계학과 ]","photoUrl":"","userId":"11292843267103243221"}},"outputId":"f2fadad4-5ab0-481f-df1f-747a85b7987f"},"source":["logits = test_sentences(['작가님 이러시면 너무 사랑합니다'])\n","prob(logits)"],"execution_count":71,"outputs":[{"output_type":"stream","text":["약 88.03% 확률로 긍정 리뷰입니다.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yLVNGmaMnnfs","executionInfo":{"status":"ok","timestamp":1606311384461,"user_tz":-540,"elapsed":835,"user":{"displayName":"‍명재성[ 대학원석사과정재학 / 통계학과 ]","photoUrl":"","userId":"11292843267103243221"}},"outputId":"051f50cf-8407-4912-8b1f-be3f852640b7"},"source":["logits = test_sentences(['전처리팀 너무 고생 많았어요'])\n","prob(logits)"],"execution_count":72,"outputs":[{"output_type":"stream","text":["약 72.84% 확률로 긍정 리뷰입니다.\n"],"name":"stdout"}]}]}