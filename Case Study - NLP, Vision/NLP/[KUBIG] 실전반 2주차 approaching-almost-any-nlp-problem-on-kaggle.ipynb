{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "49a6cbe6-f10c-4e80-b77d-47bde3848b8c",
    "_uuid": "f81f56104f6d5fc444447b41ec63aa369fd351a8"
   },
   "source": [
    "# Approaching (Almost) Any NLP Problem on Kaggle\n",
    "\n",
    "이 코드는 kaggle의 https://www.kaggle.com/abhishek/approaching-almost-any-nlp-problem-on-kaggle 코드를 가져와 리뷰한 것입니다.\n",
    "\n",
    "In this post I'll talk about approaching natural language processing problems on Kaggle. As an example, we will use the data from this competition. We will create a very basic first model first and then improve it using different other features. We will also see how deep neural networks can be used and end this post with some ideas about ensembling in general.\n",
    "\n",
    "### This covers:\n",
    "- tfidf \n",
    "- count features\n",
    "- logistic regression\n",
    "- naive bayes\n",
    "- svm\n",
    "- xgboost\n",
    "- grid search\n",
    "- word vectors\n",
    "- LSTM\n",
    "- GRU\n",
    "- Ensembling\n",
    "\n",
    "*NOTE*: This notebook is not meant for achieving a very high score on the Leaderboard for this dataset. However, if you follow it properly, you can get a very high score with some tuning. ;)\n",
    "\n",
    "So, without wasting any time, let's start with importing some important python modules that I'll be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "_cell_guid": "d46ba3fd-26f1-4635-b2f9-fca916ff3066",
    "_uuid": "21f3ccd962d1556dc2346699d45a29e9ef791367"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from tqdm import tqdm\n",
    "from sklearn.svm import SVC\n",
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.callbacks import EarlyStopping\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "60326be1-82d1-4677-8ef8-da5b1eac475c",
    "_uuid": "adb496504ab8453ce2b4f91dd6e5f17cbdaf4f68"
   },
   "source": [
    "Let's load the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "_cell_guid": "367e0329-7aeb-4f39-b1a9-d7395bdca993",
    "_uuid": "d6ea63db0ad0db09b25c35601391b71564601699"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e171d134-bb33-4578-800c-2d65c2edf9c1",
    "_uuid": "c02f2a3e039aad543bc789188fe08422dd78f5c0"
   },
   "source": [
    "A quick look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "_cell_guid": "1a9da2ba-2c8c-466d-8ceb-cb1fdb4351a8",
    "_uuid": "0fccc28ade4d126b5279d52b0f24300f8c18e69b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author\n",
       "0  id26305  This process, however, afforded me no means of...    EAP\n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL\n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP\n",
       "3  id27763  How lovely is spring As we looked from Windsor...    MWS\n",
       "4  id12958  Finding nothing else, not even gold, the Super...    HPL"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "_cell_guid": "c1ec2ba0-7bac-4983-8f68-850f26251eb6",
    "_uuid": "6b2ace3ea08492e59402dc2abcf65b99ff1a537e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id02310</td>\n",
       "      <td>Still, as I urged our leaving Ireland with suc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id24541</td>\n",
       "      <td>If a fire wanted fanning, it could readily be ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id00134</td>\n",
       "      <td>And when they had broken down the frail door t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27757</td>\n",
       "      <td>While I was thinking how I should possibly man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id04081</td>\n",
       "      <td>I am not sure to what limit his knowledge may ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text\n",
       "0  id02310  Still, as I urged our leaving Ireland with suc...\n",
       "1  id24541  If a fire wanted fanning, it could readily be ...\n",
       "2  id00134  And when they had broken down the frail door t...\n",
       "3  id27757  While I was thinking how I should possibly man...\n",
       "4  id04081  I am not sure to what limit his knowledge may ..."
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2c44d877-8fff-41a8-95dc-cd4ddebfa337",
    "_uuid": "68e1ad174df4fd40bb6965d9d46aaf42b8fa39c8"
   },
   "source": [
    "The problem requires us to predict the author, i.e. EAP, HPL and MWS given the text. In simpler words, text classification with 3 different classes.\n",
    "\n",
    "For this particular problem, Kaggle has specified multi-class log-loss as evaluation metric. This is implemented in the follow way (taken from: https://github.com/dnouri/nolearn/blob/master/nolearn/lasagne/util.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "_cell_guid": "deb46a3c-6170-4323-8fac-2710662ae0b9",
    "_uuid": "62cd92e75f858aa7c97234e8267a64b00c6d04d0"
   },
   "outputs": [],
   "source": [
    "def multiclass_logloss(actual, predicted, eps=1e-15):\n",
    "    \"\"\"Multi class version of Logarithmic Loss metric.\n",
    "    :param actual: Array containing the actual target classes\n",
    "    :param predicted: Matrix with class predictions, one probability per class\n",
    "    \"\"\"\n",
    "    # Convert 'actual' to a binary array if it's not already:\n",
    "    if len(actual.shape) == 1:\n",
    "        actual2 = np.zeros((actual.shape[0], predicted.shape[1]))\n",
    "        for i, val in enumerate(actual):\n",
    "            actual2[i, val] = 1\n",
    "        actual = actual2\n",
    "\n",
    "    clip = np.clip(predicted, eps, 1 - eps)\n",
    "    rows = actual.shape[0]\n",
    "    vsota = np.sum(actual * np.log(clip))\n",
    "    return -1.0 / rows * vsota"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://nolearndocs.readthedocs.io/en/latest/_modules/nolearn/metrics.html\n",
    "    \n",
    "- nolearn -> scikit-learn과 연동되며 기계학습에 유용한 여러 함수를 담고 있음.\n",
    "\n",
    "- 이해를 위해, 아래에서 함수가 적용된 multiclass_logloss(yvalid, predictions)를 살펴보면,\n",
    "yvalid.shape[0]가 1958이고, predictions.shape[1]이 3이므로, np.zeros((1958, 3))는 0으로 채워진 1958행 3열의 matrix를 반환하는 것이다. \n",
    "\n",
    "- yvalid: array, shape = [n_samples]\n",
    "\n",
    "- predictions: array, shape = [n_samples, n_classes]\n",
    "\n",
    "- enumerate 함수는 인덱스와 값이 같이 출력되는 함수로, 0으로 채워진 matrix에서 행-인덱스, 열-값에 해당하는 부분을 1로 바꿔준다.\n",
    "\n",
    "- np.clip(배열, 최소값 기준, 최대값 기준): 최소값과 최대값 조건으로 범위 기준을 벗어나는 값은 일괄적으로 최소값, 최대값으로 대치해준다.\n",
    "\n",
    "- le-15 = 0.000 000 000 000 001, 예측된 확률이 0인 경우, log loss값이 무한대가 된다. 그래서 실제 예측 확률은 0이라도, 15승분의 1의 값을 넣어두어 손실을 막는다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"https://miro.medium.com/max/1162/1*bUv2Dgcfw6OG9vhcpRXIeg.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 일반적인 logloss 계산 코드\n",
    "\n",
    "def logloss(true_label, predicted, eps=1e-15):\n",
    "    p = np.clip(predicted, eps, 1 - eps)\n",
    "    if true_label == 1:\n",
    "        return -log(p)\n",
    "    else:\n",
    "        return -log(1 - p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary clssification 계산 코드\n",
    "\n",
    "def logloss(true_label, predicted_prob):\n",
    "    if true_label == 1:\n",
    "        return -log(predicted_prob)\n",
    "    else:\n",
    "        return -log(1 - predicted_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"https://miro.medium.com/max/1096/1*rdBw0E-My8Gu3f_BOB6GMA.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b4a37951-7a53-43b9-bb5a-0335f1259be3",
    "_uuid": "14ede2221105fb84bb6b2d3a85f9a1f483e8b124"
   },
   "source": [
    "We use the LabelEncoder from scikit-learn to convert text labels to integers, 0, 1 2\n",
    "\n",
    "- 이 데이터가 Edgar Allan Poe /  HP Lovecraft / Mary Shelley 3명의 작가를 예측하는 문제이므로, 문자를 수치화해주는  LabelEncoder를 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "_cell_guid": "d59a646d-7739-496c-814f-594d371d76eb",
    "_uuid": "19eb8c10f06df8e0f543ee12f794df5f88b0ff1a"
   },
   "outputs": [],
   "source": [
    "lbl_enc = preprocessing.LabelEncoder()\n",
    "y = lbl_enc.fit_transform(train.author.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['EAP' 'HPL' 'MWS']\n"
     ]
    }
   ],
   "source": [
    "print(lbl_enc.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['EAP' 'HPL' 'EAP' ... 'EAP' 'EAP' 'HPL'] ==> [0 1 0 ... 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(train.author.values, \"==>\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "65403e74-091f-43c4-9523-3e15d8a75a1e",
    "_uuid": "4ffd04f40d9e921673d06ad64e01b9a7395d8e76"
   },
   "source": [
    "Before going further it is important that we split the data into training and validation sets. We can do it using `train_test_split` from the `model_selection` module of scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "_cell_guid": "ba8e606d-8dee-495e-8c3f-62aa916e9927",
    "_uuid": "b45676b121e2b719d355619e24cfed13d0d33f74"
   },
   "outputs": [],
   "source": [
    "xtrain, xvalid, ytrain, yvalid = train_test_split(train.text.values, y, \n",
    "                                                  stratify=y, \n",
    "                                                  random_state=42, \n",
    "                                                  test_size=0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- arrays : 분할시킬 데이터를 입력 (Python list, Numpy array, Pandas dataframe 등..)\n",
    "\n",
    "- test_size : 테스트 데이터셋의 비율(float)이나 갯수(int) (default = 0.25)\n",
    "\n",
    "- train_size : 학습 데이터셋의 비율(float)이나 갯수(int) (default = test_size의 나머지)\n",
    "\n",
    "- random_state : 데이터 분할시 셔플이 이루어지는데 이를 위한 시드값 (int나 RandomState로 입력)\n",
    "\n",
    "- shuffle : 셔플여부설정 (default = True)\n",
    "\n",
    "- stratify : 지정한 Data의 비율을 유지한다. 예를 들어, Label Set인 Y가 25%의 0과 75%의 1로 이루어진 Binary Set일 때, stratify=Y로 설정하면 나누어진 데이터셋들도 0과 1을 각각 25%, 75%로 유지한 채 분할된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "_cell_guid": "9e2fe6a9-8de0-4bbd-8264-f6b78e7993e2",
    "_uuid": "6c8659049537836fdf00d19d6d656630a306d217"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17621,)\n",
      "(1958,)\n"
     ]
    }
   ],
   "source": [
    "print (xtrain.shape)\n",
    "print (xvalid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3db70c26-d684-478a-bcd4-980ed6c6d65b",
    "_uuid": "794fb768f4a8e42c4be4f1dbb27144aae4d00c79"
   },
   "source": [
    "## Building Basic Models\n",
    "\n",
    "Let's start building our very first model. \n",
    "\n",
    "Our very first model is a simple TF-IDF (Term Frequency - Inverse Document Frequency) followed by a simple Logistic Regression.\n",
    "\n",
    "### 1) TfidfVectorizer\n",
    "TF-IDF는 단어를 갯수 그대로 카운트하지 않고 모든 문서에 공통적으로 들어있는 단어의 경우 문서 구별 능력이 떨어진다고 보아 가중치를 축소하는 방법\n",
    "\n",
    "단어 빈도 또는 등장 여부를 그대로 임베딩으로 쓰는 것에는 단점이 있음. 어떤 문서에든 쓰여서 해당 단어가 많이 나타났다 하더라도 문서의 주제를 가늠하기 어려운 경우가 있기 때문. Tfidf는 이러한 단점을 보완한다.\n",
    "\n",
    "- TF: 어떤 단어가 특정 문서에서 얼마나 많이 쓰였는지\n",
    "- DF: 특정 단어가 나타난 문서의 수\n",
    "- IDF: 전체 문서 수를 해당 단어의 DF로 나눈뒤 로그를 취한 값, 그 값이 클수록 특이한 단어"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"https://t1.daumcdn.net/cfile/tistory/22346248538D3D1205\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "_cell_guid": "b387f2af-11b1-455d-ad8d-320ed1005be3",
    "_uuid": "350d453dc982f494c3774dbdcf731d856546d611"
   },
   "outputs": [],
   "source": [
    "# Always start with these features. They work (almost) everytime!\n",
    "tfv = TfidfVectorizer(min_df=3,  max_features=None, \n",
    "            strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
    "            ngram_range=(1, 3), use_idf=1, smooth_idf=1, sublinear_tf=1,\n",
    "            stop_words = 'english')\n",
    "\n",
    "# Fitting TF-IDF to both training and test sets (semi-supervised learning)\n",
    "tfv.fit(list(xtrain) + list(xvalid))\n",
    "xtrain_tfv =  tfv.transform(xtrain) \n",
    "xvalid_tfv = tfv.transform(xvalid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- mid-df : DF(document-frequency, 문서의 수)의 최소 빈도값을 설정, 해당 값보다 작은 DF를 가진 단어들은 단어사전(vocabulary_)에서 제외하고, 인덱스를 부여하지 않음\n",
    "\n",
    "- max_features: 최대 feature를 설정, 다른 데이터의 단어들은 10 정도를 가지는데, 어떤 데이터만 단어 종류가 100이 넘어간다고 하면, 이 100에 맞추어 feature의 수가 엄청 늘어나게 된다. 이 경우, 모델 성능이 저하될 수도 있다.\n",
    "\n",
    "- strip_accents : 문자 정규화, 'ascii', 'unicode'가 있는데, 'unicode'가 느리지만, 모든 문자에 적용 가능한 방법이다. default = None\n",
    "\n",
    "- analyzer : 학습단위를 결정, 'word'라고 설정시 학습 단위를 단어로 설정하고, 'char'로 설정시 학습 단위를 글자로 설정\n",
    "\n",
    "- token_pattern : 토큰을 구성하는 정규표현식 정의\n",
    "\n",
    "- ngram_range : 단어 묶음 범위를 결정, (1, 1)이라면 단어의 묶음을 1개부터 1개까지 설정하라는 뜻으로, 기존과 차이가 없다. (1, 2)라면, 단어의 묶음을 1개부터 2개까지 설정하라는 뜻으로, 단어 사전에는 1개 단어 묶음과 2개 단어 묶음이 모두 존재하게 된다. (very good)\n",
    "\n",
    "- use_idf : inverse-document-frequency(특정한 단어가 들어있는 문서의 수에 반비례하는 수) 활용 여부를 결정, default = True\n",
    "\n",
    "- smooth_idf : idf 가중치의 스무딩(smoothing) 여부를 결정\n",
    "\n",
    "- sublinear_tf : TF(Term-frequency, 단어 빈도)의 스무딩(smooothing) 여부를 결정, True/False\n",
    "\n",
    "- stop_words : 불용어 설정, default = None\n",
    "\n",
    "-> 불용어 제거 : 갖고 있는 데이터에서 유의미한 단어 토큰만을 선별하기 위해서는 큰 의미가 없는 단어 토큰을 제거하는 작업, I, my, me, over, 조사, 접미사 같은 단어들은 문장에서는 자주 등장하지만 실제 의미 분석을 하는데는 거의 기여하는 바가 없는 경우가 많다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.605170185988092\n",
      "10.210340371976184\n"
     ]
    }
   ],
   "source": [
    "def sublinear_func(input): \n",
    "    rst = 1 + np.log(input) \n",
    "    return rst \n",
    "\n",
    "print(sublinear_func(100)) \n",
    "print(sublinear_func(10000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 결과를 보면 100은 5.6 정도로, 10000은 10.2 정도로 값이 확 줄어든 것을 확인할 수 있다.\n",
    "이처럼 sublinear_tf는 높은 TF값을 완만하게 처리하는 효과를 가지고 있다.\n",
    "TF의 아웃라이어가 너무 심한 데이터의 경우, 이 파라미터를 True로 바꿔주면 어느정도 효과를 기대할 수 있다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "_cell_guid": "4106bbd1-dc35-4dc2-bda0-3024d3c056d3",
    "_uuid": "3f5dd9ce043364fc61ba3a30298acd9cb72a2938"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.572 \n"
     ]
    }
   ],
   "source": [
    "# Fitting a simple Logistic Regression on TFIDF\n",
    "clf = LogisticRegression(C=1.0)\n",
    "clf.fit(xtrain_tfv, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_tfv)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "61b2dcb5-46dd-4666-b27f-a8bb48b07f79",
    "_uuid": "f347acb0850db4d13d355e8238250e64f6fbcde8"
   },
   "source": [
    "And there we go. We have our first model with a multiclass logloss of 0.626.\n",
    "\n",
    "But we are greedy and want a better score. Lets look at the same model with a different data.\n",
    "\n",
    "Instead of using TF-IDF, we can also use word counts as features. This can be done easily using CountVectorizer from scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) CountVectorizer\n",
    "\n",
    "1. 문서를 토큰 리스트로 변환한다.\n",
    "2. 각 문서에서 토큰의 출현 빈도를 센다.\n",
    "3. 각 문서를 BOW 인코딩 벡터로 변환한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"https://www.educative.io/api/edpresso/shot/5197621598617600/image/6596233398321152\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "_cell_guid": "7605227c-6cf5-4e00-bcf9-5781afdcc5ed",
    "_uuid": "b464626583417155f1d746e44a2303f560506706"
   },
   "outputs": [],
   "source": [
    "ctv = CountVectorizer(analyzer='word',token_pattern=r'\\w{1,}',\n",
    "            ngram_range=(1, 3), stop_words = 'english')\n",
    "\n",
    "# Fitting Count Vectorizer to both training and test sets (semi-supervised learning)\n",
    "ctv.fit(list(xtrain) + list(xvalid))\n",
    "xtrain_ctv =  ctv.transform(xtrain) \n",
    "xvalid_ctv = ctv.transform(xvalid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- stop_words : stop words 목록.‘english’이면 영어 불용어 목록 사용, default = None\n",
    "\n",
    "- analyzer : 학습 단위를 결정, {‘word’, ‘char’, ‘char_wb’} - {단어 n-그램, 문자 n-그램, 단어 내의 문자 n-그램}, 혹은 함수\n",
    "\n",
    "- token_pattern : 토큰을 구성하는 정규표현식 정의\n",
    "\n",
    "- tokenizer : 토큰 생성 함수, default = None\n",
    "\n",
    "- ngram_range : (min_n, max_n) 튜플, n-그램 범위\n",
    "\n",
    "- max_df : 정수 또는 [0.0, 1.0] 사이의 실수. 단어 사전에 포함되기 위한 최대 빈도, default = 1\n",
    "\n",
    "- min_df : 정수 또는 [0.0, 1.0] 사이의 실수. 단어 사전에 포함되기 위한 최소 빈도, default = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "_cell_guid": "fb008df6-95ba-4f50-bdb0-f18371253d39",
    "_uuid": "e459be88b56b9a238b7e7bddfd52df1576dd7dce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.527 \n"
     ]
    }
   ],
   "source": [
    "# Fitting a simple Logistic Regression on Counts\n",
    "clf = LogisticRegression(C=1.0)\n",
    "clf.fit(xtrain_ctv, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_ctv)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "be77d9bc-e052-47a5-aec6-12a0e07df407",
    "_uuid": "e2eb975dda65d0e32e1160843c54eba171b93f9b"
   },
   "source": [
    "Aaaaanddddddd Wallah! We just improved our first model by 0.1!!!\n",
    "\n",
    "Next, let's try a very simple model which was quite famous in ancient times - Naive Bayes.\n",
    "\n",
    "Let's see what happens when we use naive bayes on these two datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "_cell_guid": "e306fce8-89f5-4c87-9fb8-e25445f3d778",
    "_uuid": "111c474e5cadf662f8f714d689c1079e2730b15b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.578 \n"
     ]
    }
   ],
   "source": [
    "# Fitting a simple Naive Bayes on TFIDF\n",
    "clf = MultinomialNB()\n",
    "clf.fit(xtrain_tfv, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_tfv)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c62901ef-dd3b-43dd-87ee-7d365138878a",
    "_uuid": "a629ee2a38f9944ad874c067a089c9957c780059"
   },
   "source": [
    "Good performance! But the logistic regression on counts is still better! What happens when we use this model on counts data instead?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "_cell_guid": "db5ca297-b2a8-47fc-beec-90727e32d169",
    "_uuid": "8a91cb086f7fa95281d4c7a795e60f20a22ba865"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.485 \n"
     ]
    }
   ],
   "source": [
    "# Fitting a simple Naive Bayes on Counts\n",
    "clf = MultinomialNB()\n",
    "clf.fit(xtrain_ctv, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_ctv)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5647bb2d-8d76-4a98-91ec-8b111a7a5b30",
    "_uuid": "0c9a2e87e332839f37e2a53bb88a876bb1520cd7"
   },
   "source": [
    "Whoa! Seems like old stuff still works good!!!! One more ancient algorithms in the list is SVMs. Some people \"love\" SVMs. So, we must try SVM on this dataset.\n",
    "\n",
    "Since SVMs take a lot of time, we will reduce the number of features from the TF-IDF using Singular Value Decomposition before applying SVM. \n",
    "\n",
    "Also, note that before applying SVMs, we *must* standardize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "_cell_guid": "ae84d757-601f-4f7f-b53d-6d4f99ae7632",
    "_uuid": "d055b28969acd7a93afbb7c51c187b74e539e01b"
   },
   "outputs": [],
   "source": [
    "# Apply SVD, I chose 120 components. 120-200 components are good enough for SVM model.\n",
    "svd = decomposition.TruncatedSVD(n_components=120)\n",
    "svd.fit(xtrain_tfv)\n",
    "xtrain_svd = svd.transform(xtrain_tfv)\n",
    "xvalid_svd = svd.transform(xvalid_tfv)\n",
    "\n",
    "# Scale the data obtained from SVD. Renaming variable to reuse without scaling.\n",
    "scl = preprocessing.StandardScaler()\n",
    "scl.fit(xtrain_svd)\n",
    "xtrain_svd_scl = scl.transform(xtrain_svd)\n",
    "xvalid_svd_scl = scl.transform(xvalid_svd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"https://wikidocs.net/images/page/24949/svd%EC%99%80truncatedsvd.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- atent Semantic Indexing -> 특이값 분해 계산: 차원 축소\n",
    "\n",
    "동의어나 다의어로부터 오는 문제를 해결하기 위해 정보 검색 분야에 널리 사용되고 있다. \n",
    "\n",
    "TruncatedSVD를 통해 데이터를 중요순서대로 끊어내는 프로세스를 할 수 있다.\n",
    "절단된 SVD는 대각 행렬 Σ의 대각 원소의 값 중에서 상위값 t개만 남게 된다. t는 우리가 찾고자하는 토픽의 수를 반영한 하이퍼파라미터 값이다. t를 선택하는 것은 쉽지 않다. t를 크게 잡으면 기존의 행렬 A로부터 다양한 의미를 가져갈 수 있지만, t를 작게 잡아야만 노이즈를 제거할 수 있기 때문이다.\n",
    "\n",
    "자연어 처리 분야에서는 위와 같은 과정을 통해, 설명력이 낮은 정보를 삭제하고 설명력이 높은 정보를 남길 수 있다.\n",
    "\n",
    "한편, SVM은 scaling에 민감하기 때문에, 평균을 0, 표준표차가 1이 되도록 하는 standardscaling을 진행한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "54f2755a-be45-4288-a11f-4e485b39b362",
    "_uuid": "3786fbe2fb370926769f27ef38af948aec0ce78b"
   },
   "source": [
    "Now it's time to apply SVM. After running the following cell, feel free to go for a walk or talk to your girlfriend/boyfriend. :P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "_cell_guid": "93f65421-ccde-4711-a292-63ce5d8a1f5f",
    "_uuid": "293f90a7524ab90753ecae42244d1337799740dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.740 \n"
     ]
    }
   ],
   "source": [
    "# Fitting a simple SVM\n",
    "clf = SVC(C=1.0, probability=True) # since we need probabilities\n",
    "clf.fit(xtrain_svd_scl, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_svd_scl)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f34d307a-ddc4-4c7e-9034-2ffc59cd73d3",
    "_uuid": "534f762addade24a91d0c37cebb195e510e1a6cd"
   },
   "source": [
    "Oops! time to get up! Looks like SVM doesn't perform well on this data...! \n",
    "\n",
    "Before moving further, lets apply the most popular algorithm on Kaggle: xgboost!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "_cell_guid": "b8bf1058-a784-4d9e-b55b-b33b86bdccde",
    "_uuid": "b896de07f6a4901c5b8896f94edf0ddbdf072137"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.782 \n"
     ]
    }
   ],
   "source": [
    "# Fitting a simple xgboost on tf-idf\n",
    "clf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n",
    "                        subsample=0.8, nthread=10, learning_rate=0.1)\n",
    "clf.fit(xtrain_tfv.tocsc(), ytrain)\n",
    "predictions = clf.predict_proba(xvalid_tfv.tocsc())\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "_cell_guid": "e774a72f-a605-4cce-b68f-0005dd260c58",
    "_uuid": "9c62f03cadaa936f5cba3936e8a05697009c2575"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.773 \n"
     ]
    }
   ],
   "source": [
    "# Fitting a simple xgboost on tf-idf\n",
    "clf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n",
    "                        subsample=0.8, nthread=10, learning_rate=0.1)\n",
    "clf.fit(xtrain_ctv.tocsc(), ytrain)\n",
    "predictions = clf.predict_proba(xvalid_ctv.tocsc())\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "_cell_guid": "5aa042f9-7871-4a2a-bfb4-359b39d7df39",
    "_uuid": "df266177145ab4ad9c291bbb3874cff687def3b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.783 \n"
     ]
    }
   ],
   "source": [
    "# Fitting a simple xgboost on tf-idf svd features\n",
    "clf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n",
    "                        subsample=0.8, nthread=10, learning_rate=0.1)\n",
    "clf.fit(xtrain_svd, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_svd)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "_cell_guid": "39d1153f-f890-4a37-8d99-317c9bf84c6e",
    "_uuid": "43042fb84ef91e317cfcfa76cbed3d7a02439a03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.789 \n"
     ]
    }
   ],
   "source": [
    "# Fitting a simple xgboost on tf-idf svd features\n",
    "clf = xgb.XGBClassifier(nthread=10)\n",
    "clf.fit(xtrain_svd, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_svd)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- XGBoost(eXtra Gradient Boost): 트리 기반의 알고리즘의 앙상블 학습에서 각광받는 알고리즘 \n",
    "    \n",
    "- max_depth : 트리 기반 알고리즘의 max_depth와 동일, 0을 지정하면 깊이의 제한이 없음, 너무 크면 과적합(통상 3~10정도 적용)\n",
    "\n",
    "- n_estimators : 생성할 weak learner의 수\n",
    "\n",
    "- colsample_bytree : GBM(그레디언트 부스트)의 max_features와 유사, 트리 생성에 필요한 피처의 샘플링에 사용, 피처가 많을 때 과적합 조절에 사용, 범위: 0 ~ 1\n",
    "\n",
    "- subsample : GBM의 subsample과 동일, 데이터 샘플링 비율 지정(과적합 제어), 일반적으로 0.5~1 사이의 값을 사용, 범위: 0 ~ 1\n",
    "\n",
    "- nthread : CPU 실행 스레드 개수 조정, Default는 전체 다 사용하는 것, 멀티코어/스레드 CPU 시스템에서 일부CPU만 사용할 때 변경\n",
    "\n",
    "- learning_rate : 학습률 결정, 범위 0~1\n",
    "\n",
    "\n",
    "트리 모델의 중요 매개변수는 트리의 개수를 지정하는 n_estimators와 이전 트리의 오차를 보정하는 정도를 조절하는 learning_rate이다. 이 두 매개변수는 매우 깊게 연관되며 learning_rate를 낮추면 비슷한 복잡도의 모델을 만들기 위해서 더 많은 트리를 추가해야 한다. n_estimators가 클수록 좋은 랜덤 포레스트와는 달리 그래디언트 부스팅에서 n_estimators를 크게 하면 모델이 복잡해지고 과대적합될 가능성이 높아진다. 일반적인 관례는 가용한 시간과 메모리 한도에서 n_estimators를 맞추고 나서 적절한 learning_rate를 찾는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ed048511-1fbb-46d8-8e83-8e4f2198f30f",
    "_uuid": "49d47f7751496c1b8967ff00158df82a0e5e9f18"
   },
   "source": [
    "Seems like no luck with XGBoost! But that is not correct. I haven't done any hyperparameter optimizations yet. And since I'm lazy, I'll just tell you how to do it and you can do it on your own! ;). This will be discussed in the next section:\n",
    "\n",
    "\n",
    "## Grid Search\n",
    "\n",
    "Its a technique for hyperparameter optimization. Not so effective but can give good results if you know the grid you want to use. I specify the parameters that should usually be used in this post: http://blog.kaggle.com/2016/07/21/approaching-almost-any-machine-learning-problem-abhishek-thakur/ Please keep in mind that these are the parameters I usually use. There are many other methods of hyperparameter optimization which may or may not be as effective.\n",
    "\n",
    "In this section, I'll talk about grid search using logistic regression. \n",
    "\n",
    "Before starting with grid search we need to create a scoring function. This is accomplished using the `make_scorer` function of scikit-learn.\n",
    "\n",
    "\n",
    "- 그리드 서치는 관심 있는 매개변수들을 대상으로 가능한 모든 조합들을 시도하여 초적의 매개변수를 찾는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "_cell_guid": "50a7703b-52e4-4057-8af7-0d0492f05450",
    "_uuid": "7c3148c857c62662522fddd37d48642526f878da"
   },
   "outputs": [],
   "source": [
    "mll_scorer = metrics.make_scorer(multiclass_logloss, greater_is_better=False, needs_proba=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "93caaa31-acfc-456d-a991-e37e0279b88f",
    "_uuid": "ee1ec5f4ec8d8db4c888124f63b9aa3ccb5ca736"
   },
   "source": [
    "Next we need a pipeline. For demonstration here, i'll be using a pipeline consisting of SVD, scaling and then logistic regression. Its better to understand with more modules in pipeline than just one ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- sklearn에서는 custom metric으로 모델을 선정할 수 있다.\n",
    "\n",
    "- greater_is_better : score function이 높으면 높을 수록 좋음, 만약 손실 함수일 경우 낮음이 좋음.\n",
    "\n",
    "- needs_proba : 분류 모델에서 predict_proba를 요구하는지 여부"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "_cell_guid": "0fd480dd-71da-4efc-a8e0-cabc3ab75110",
    "_uuid": "26404a6fbd2dbd045bd4af3fdc6317f6ca1f789f"
   },
   "outputs": [],
   "source": [
    "# Initialize SVD\n",
    "svd = TruncatedSVD()\n",
    "    \n",
    "# Initialize the standard scaler \n",
    "scl = preprocessing.StandardScaler()\n",
    "\n",
    "# We will use logistic regression here..\n",
    "lr_model = LogisticRegression()\n",
    "\n",
    "# Create the pipeline \n",
    "clf = pipeline.Pipeline([('svd', svd),\n",
    "                         ('scl', scl),\n",
    "                         ('lr', lr_model)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "60857c91-6555-473f-a21d-4ddf565b14d7",
    "_uuid": "343935d35ebf9f36d7be0231fe1cbe9d567be48d"
   },
   "source": [
    "Next we need a grid of parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "_cell_guid": "0d935141-9595-4334-90e5-8ba00281d94c",
    "_uuid": "b3704ca308ccf25b3ea169f923cb0e57cc66f2b6"
   },
   "outputs": [],
   "source": [
    "param_grid = {'svd__n_components' : [120, 180],\n",
    "              'lr__C': [0.1, 1.0, 10], \n",
    "              'lr__penalty': ['l1', 'l2']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9acb22f4-31d3-436c-bdeb-cf7ba6156ae3",
    "_uuid": "c36718c732d600c08ca82718956e8933183ccd09"
   },
   "source": [
    "So, for SVD we evaluate 120 and 180 components and for logistic regression we evaluate three different values of C with l1 and l2 penalty. We can now start grid search on these parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "_cell_guid": "af0549e8-dc18-4006-a859-af0e36724d1d",
    "_uuid": "10aec7cf5d8c3a1d4ccaf514302c1866eea1b152"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 12 candidates, totalling 24 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    3.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of  24 | elapsed:    3.3s remaining:   36.7s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of  24 | elapsed:    4.4s remaining:   16.5s\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of  24 | elapsed:    4.6s remaining:    9.3s\n",
      "[Parallel(n_jobs=-1)]: Done  11 out of  24 | elapsed:    5.3s remaining:    6.3s\n",
      "[Parallel(n_jobs=-1)]: Done  14 out of  24 | elapsed:    6.0s remaining:    4.3s\n",
      "[Parallel(n_jobs=-1)]: Done  17 out of  24 | elapsed:    6.7s remaining:    2.7s\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  24 | elapsed:    7.4s remaining:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:    8.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: -0.740\n",
      "Best parameters set:\n",
      "\tlr__C: 0.1\n",
      "\tlr__penalty: 'l2'\n",
      "\tsvd__n_components: 180\n"
     ]
    }
   ],
   "source": [
    "# Initialize Grid Search Model\n",
    "model = GridSearchCV(estimator=clf, param_grid=param_grid, scoring=mll_scorer,\n",
    "                                 verbose=10, n_jobs=-1, iid=True, refit=True, cv=2)\n",
    "\n",
    "# Fit Grid Search Model\n",
    "model.fit(xtrain_tfv, ytrain)  # we can use the full data here but im only using xtrain\n",
    "print(\"Best score: %0.3f\" % model.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = model.best_estimator_.get_params()\n",
    "for param_name in sorted(param_grid.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- param_grid : 찾고자 하는 파라미터, dict 형식\n",
    "\n",
    "- verbose: 상세 정보를 보여주는 정도를 선택 가능, 높을수록 더 많은 메시지가 표시\n",
    "\n",
    "- n_jobs : 벙렬처리 갯수, default는 1, 이 값을 증가시키면 내부적으로 멀티 프로세스를 사용하여 그리드서치를 수행. 만약 CPU 코어의 수가 충분하다면 n_jobs를 늘릴 수록 속도가 증가\n",
    "\n",
    "- iid : True로 설정 시, data가 전체에 동일하게 배포되는 것으로 간주, loss는 샘플 당 전체 loss를 감소시킴\n",
    "\n",
    "- refit : default가 True, 좋은 estimator로 수정되어짐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3bd30250-1894-4fbc-9306-bb2544d5ae10",
    "_uuid": "8ee28a0f484d2e4c6768f3fcd78dd4a017e013fc"
   },
   "source": [
    "The score comes similar to what we had for SVM. This technique can be used to finetune xgboost or even multinomial naive bayes as below. We will use the tfidf data here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "_cell_guid": "444bb821-e8ce-4efd-ab5c-cdb457a438a0",
    "_uuid": "154fb55edb63f5d0c70471a27b2baaa3fa373efa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 6 candidates, totalling 12 fits\n",
      "Best score: -0.492\n",
      "Best parameters set:\n",
      "\tnb__alpha: 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.0256s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of  12 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of  12 | elapsed:    0.1s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  12 | elapsed:    0.1s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of  12 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "nb_model = MultinomialNB()\n",
    "\n",
    "# Create the pipeline \n",
    "clf = pipeline.Pipeline([('nb', nb_model)])\n",
    "\n",
    "# parameter grid\n",
    "param_grid = {'nb__alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "# Initialize Grid Search Model\n",
    "model = GridSearchCV(estimator=clf, param_grid=param_grid, scoring=mll_scorer,\n",
    "                                 verbose=10, n_jobs=-1, iid=True, refit=True, cv=2)\n",
    "\n",
    "# Fit Grid Search Model\n",
    "model.fit(xtrain_tfv, ytrain)  # we can use the full data here but im only using xtrain. \n",
    "print(\"Best score: %0.3f\" % model.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = model.best_estimator_.get_params()\n",
    "for param_name in sorted(param_grid.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "01cee065-8733-44d6-b348-7124e178c6e5",
    "_uuid": "ecd0f67a617ae3c03f2386dd722be343c2384ff3"
   },
   "source": [
    "This is an improvement of 8% over the original naive bayes score!\n",
    "\n",
    "In NLP problems, it's customary to look at word vectors. Word vectors give a lot of insights about the data. Let's dive into that.\n",
    "\n",
    "## Word Vectors\n",
    "\n",
    "Without going into too much details, I would explain how to create sentence vectors and how can we use them to create a machine learning model on top of it. I am a fan of GloVe vectors, word2vec and fasttext. In this post, I'll be using the GloVe vectors. You can download the GloVe vectors from here `http://www-nlp.stanford.edu/data/glove.840B.300d.zip`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- glove: word2vec의 단점을 해결하기 위해, 미국 스탠포드 대학에서 개발한 방법론\n",
    "\n",
    "- word2vec은 실제값과 예측값에 대한 오차를 손실 함수를 통해 줄여나가며 학습하는 예측 기반의 방법론이다. 중심 단어로 주변 단어, 주변 단어로 중심 단어를 예측하는 과정에서 단어를 벡터화하는 것으로, 임베딩된 단어의 내적이 코사인 유사도가 되도록 한다. \n",
    "\n",
    "- 연구진은 단어의 공동 출현 횟수를 세어서 정방행렬 형태로 기록한 후, 그 행렬에 SVD를 적용해 2개의 가중치 행렬을 얻었다. glove는 임베딩된 중심 단어와 주변 단어 벡터의 내적이 전체 코퍼스에서의 동시 등장 확률이 되도록 한다. 이 기법은 말뭉치 전체에 대한 동시 등장 확률 global vector를 최적화하는 방법론이다. 이를 통해, word2vec의 은닉 가중치 행렬과 출력 가중치 행렬을 훨씬 짧은 시간으로 산출할 수 있다.\n",
    "\n",
    "- \"임베딩된 단어 벡터 간 유사도 측정을 수월하게 하면서도 말뭉치 전체의 통계 정보를 좀 더 잘 반영해보자\"가 glove의 목표이다.\n",
    "\n",
    "- word2vec과 glove 모두 핵심은 단순히 단어를 'count'하는 방식이 아니라 단어의 '동시 등장 여부'를 확률 기반으로 예측한다는 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the GloVe vectors in a dictionary:\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open('glove.840B.300d.txt')\n",
    "for line in tqdm(f):\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "_cell_guid": "fe4e61d9-28d9-4171-a589-b1effaaa1116",
    "_uuid": "a7445bd63a857a4619043f8916384d038a9f5c54"
   },
   "outputs": [],
   "source": [
    "# this function creates a normalized vector for the whole sentence\n",
    "def sent2vec(s):\n",
    "    words = str(s).lower().encode().decode('utf-8')\n",
    "    words = word_tokenize(words)\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    M = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(embeddings_index[w])\n",
    "        except:\n",
    "            continue\n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    if type(v) != np.ndarray:\n",
    "        return np.zeros(300)\n",
    "    return v / np.sqrt((v ** 2).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- isalpha()는 문자열이 문자인지 아닌지를 True, Flase로 리턴해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "_cell_guid": "e79158dc-eb32-416a-ae91-d38612fca82f",
    "_uuid": "c45f5aeb64f55b6affa932b2f0048b058264518e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/17621 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|▏         | 221/17621 [00:00<00:07, 2206.43it/s]\u001b[A\n",
      "  3%|▎         | 474/17621 [00:00<00:07, 2293.85it/s]\u001b[A\n",
      "  4%|▍         | 725/17621 [00:00<00:07, 2353.46it/s]\u001b[A\n",
      "  5%|▌         | 920/17621 [00:00<00:07, 2214.57it/s]\u001b[A\n",
      "  7%|▋         | 1189/17621 [00:00<00:07, 2337.74it/s]\u001b[A\n",
      "  8%|▊         | 1443/17621 [00:00<00:06, 2393.77it/s]\u001b[A\n",
      " 10%|▉         | 1709/17621 [00:00<00:06, 2466.07it/s]\u001b[A\n",
      " 11%|█▏        | 1986/17621 [00:00<00:06, 2548.51it/s]\u001b[A\n",
      " 13%|█▎        | 2231/17621 [00:00<00:06, 2442.92it/s]\u001b[A\n",
      " 14%|█▍        | 2470/17621 [00:01<00:06, 2424.60it/s]\u001b[A\n",
      " 15%|█▌        | 2726/17621 [00:01<00:06, 2461.81it/s]\u001b[A\n",
      " 17%|█▋        | 2981/17621 [00:01<00:05, 2486.64it/s]\u001b[A\n",
      " 18%|█▊        | 3247/17621 [00:01<00:05, 2535.16it/s]\u001b[A\n",
      " 20%|█▉        | 3519/17621 [00:01<00:05, 2586.45it/s]\u001b[A\n",
      " 21%|██▏       | 3784/17621 [00:01<00:05, 2604.69it/s]\u001b[A\n",
      " 23%|██▎       | 4048/17621 [00:01<00:05, 2613.62it/s]\u001b[A\n",
      " 25%|██▍       | 4324/17621 [00:01<00:05, 2651.72it/s]\u001b[A\n",
      " 26%|██▌       | 4602/17621 [00:01<00:04, 2688.85it/s]\u001b[A\n",
      " 28%|██▊       | 4877/17621 [00:01<00:04, 2701.62it/s]\u001b[A\n",
      " 29%|██▉       | 5149/17621 [00:02<00:04, 2703.67it/s]\u001b[A\n",
      " 31%|███       | 5420/17621 [00:02<00:04, 2686.16it/s]\u001b[A\n",
      " 32%|███▏      | 5689/17621 [00:02<00:04, 2647.76it/s]\u001b[A\n",
      " 34%|███▍      | 5954/17621 [00:02<00:04, 2644.82it/s]\u001b[A\n",
      " 35%|███▌      | 6219/17621 [00:02<00:04, 2644.81it/s]\u001b[A\n",
      " 37%|███▋      | 6484/17621 [00:02<00:04, 2645.60it/s]\u001b[A\n",
      " 38%|███▊      | 6749/17621 [00:02<00:04, 2643.07it/s]\u001b[A\n",
      " 40%|███▉      | 7014/17621 [00:02<00:04, 2637.46it/s]\u001b[A\n",
      " 41%|████▏     | 7281/17621 [00:02<00:03, 2644.88it/s]\u001b[A\n",
      " 43%|████▎     | 7546/17621 [00:02<00:03, 2606.15it/s]\u001b[A\n",
      " 44%|████▍     | 7811/17621 [00:03<00:03, 2617.20it/s]\u001b[A\n",
      " 46%|████▌     | 8078/17621 [00:03<00:03, 2631.84it/s]\u001b[A\n",
      " 47%|████▋     | 8352/17621 [00:03<00:03, 2662.57it/s]\u001b[A\n",
      " 49%|████▉     | 8621/17621 [00:03<00:03, 2668.84it/s]\u001b[A\n",
      " 50%|█████     | 8896/17621 [00:03<00:03, 2691.16it/s]\u001b[A\n",
      " 52%|█████▏    | 9166/17621 [00:03<00:03, 2662.65it/s]\u001b[A\n",
      " 54%|█████▎    | 9433/17621 [00:03<00:03, 2636.93it/s]\u001b[A\n",
      " 55%|█████▌    | 9697/17621 [00:03<00:03, 2627.12it/s]\u001b[A\n",
      " 57%|█████▋    | 9965/17621 [00:03<00:02, 2638.56it/s]\u001b[A\n",
      " 58%|█████▊    | 10229/17621 [00:03<00:02, 2589.89it/s]\u001b[A\n",
      " 60%|█████▉    | 10489/17621 [00:04<00:02, 2573.87it/s]\u001b[A\n",
      " 61%|██████    | 10747/17621 [00:04<00:02, 2564.11it/s]\u001b[A\n",
      " 62%|██████▏   | 11004/17621 [00:04<00:02, 2522.18it/s]\u001b[A\n",
      " 64%|██████▍   | 11257/17621 [00:04<00:02, 2316.28it/s]\u001b[A\n",
      " 65%|██████▌   | 11493/17621 [00:04<00:02, 2094.13it/s]\u001b[A\n",
      " 67%|██████▋   | 11745/17621 [00:04<00:02, 2205.41it/s]\u001b[A\n",
      " 68%|██████▊   | 12006/17621 [00:04<00:02, 2312.80it/s]\u001b[A\n",
      " 70%|██████▉   | 12265/17621 [00:04<00:02, 2389.25it/s]\u001b[A\n",
      " 71%|███████   | 12526/17621 [00:04<00:02, 2450.01it/s]\u001b[A\n",
      " 73%|███████▎  | 12786/17621 [00:05<00:01, 2491.99it/s]\u001b[A\n",
      " 74%|███████▍  | 13059/17621 [00:05<00:01, 2558.46it/s]\u001b[A\n",
      " 76%|███████▌  | 13318/17621 [00:05<00:01, 2520.45it/s]\u001b[A\n",
      " 77%|███████▋  | 13579/17621 [00:05<00:01, 2545.27it/s]\u001b[A\n",
      " 79%|███████▊  | 13843/17621 [00:05<00:01, 2570.58it/s]\u001b[A\n",
      " 80%|████████  | 14120/17621 [00:05<00:01, 2626.96it/s]\u001b[A\n",
      " 82%|████████▏ | 14397/17621 [00:05<00:01, 2666.09it/s]\u001b[A\n",
      " 83%|████████▎ | 14688/17621 [00:05<00:01, 2733.54it/s]\u001b[A\n",
      " 85%|████████▍ | 14963/17621 [00:05<00:00, 2719.91it/s]\u001b[A\n",
      " 86%|████████▋ | 15242/17621 [00:05<00:00, 2739.64it/s]\u001b[A\n",
      " 88%|████████▊ | 15517/17621 [00:06<00:00, 2684.26it/s]\u001b[A\n",
      " 90%|████████▉ | 15787/17621 [00:06<00:00, 2663.97it/s]\u001b[A\n",
      " 91%|█████████ | 16054/17621 [00:06<00:00, 2627.06it/s]\u001b[A\n",
      " 93%|█████████▎| 16318/17621 [00:06<00:00, 2629.64it/s]\u001b[A\n",
      " 94%|█████████▍| 16582/17621 [00:06<00:00, 2614.07it/s]\u001b[A\n",
      " 96%|█████████▌| 16844/17621 [00:06<00:00, 2595.36it/s]\u001b[A\n",
      " 97%|█████████▋| 17104/17621 [00:06<00:00, 2572.16it/s]\u001b[A\n",
      "100%|██████████| 17621/17621 [00:06<00:00, 2569.66it/s]\u001b[A\n",
      "\n",
      "  0%|          | 0/1958 [00:00<?, ?it/s]\u001b[A\n",
      " 13%|█▎        | 254/1958 [00:00<00:00, 2539.52it/s]\u001b[A\n",
      " 25%|██▌       | 499/1958 [00:00<00:00, 2511.40it/s]\u001b[A\n",
      " 38%|███▊      | 753/1958 [00:00<00:00, 2519.43it/s]\u001b[A\n",
      " 49%|████▉     | 960/1958 [00:00<00:00, 2364.32it/s]\u001b[A\n",
      " 60%|█████▉    | 1166/1958 [00:00<00:00, 2263.40it/s]\u001b[A\n",
      " 69%|██████▉   | 1352/1958 [00:00<00:00, 2003.45it/s]\u001b[A\n",
      " 81%|████████  | 1586/1958 [00:00<00:00, 2092.75it/s]\u001b[A\n",
      "100%|██████████| 1958/1958 [00:00<00:00, 2267.60it/s]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "# create sentence vectors using the above function for training and validation set\n",
    "xtrain_glove = [sent2vec(x) for x in tqdm(xtrain)]\n",
    "xvalid_glove = [sent2vec(x) for x in tqdm(xvalid)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "_cell_guid": "ac1c5c55-a5ca-4f1f-a5cc-ff0964f887d8",
    "_uuid": "e25a2275ca165dc4e90589f4778c9b84585448a1"
   },
   "outputs": [],
   "source": [
    "xtrain_glove = np.array(xtrain_glove)\n",
    "xvalid_glove = np.array(xvalid_glove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9f2b9d5f-d31c-4ed2-ba7f-43833bd63ddd",
    "_uuid": "a295bf47c480c9112dee20e5691594b379f4512e"
   },
   "source": [
    "Let's see the performance of xgboost on glove features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "_cell_guid": "fb5bb25f-eac2-457d-b69d-e236b5d47953",
    "_uuid": "e35e50e524ae353dda1614c758ff59901e5af23c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.791 \n"
     ]
    }
   ],
   "source": [
    "# Fitting a simple xgboost on glove features\n",
    "clf = xgb.XGBClassifier(nthread=10, silent=False)\n",
    "clf.fit(xtrain_glove, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_glove)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "_cell_guid": "41fa545d-0576-4539-bc8c-fb03aef61440",
    "_uuid": "8ab2c7e8eb07e9eb6b7b21d36dcd9b366672cac2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.747 \n"
     ]
    }
   ],
   "source": [
    "# Fitting a simple xgboost on glove features\n",
    "clf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n",
    "                        subsample=0.8, nthread=10, learning_rate=0.1, silent=False)\n",
    "clf.fit(xtrain_glove, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_glove)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "12187a0c-ec9d-46ba-84bb-d99ff9850cf0",
    "_uuid": "f2e77eaf67811c2b8e1465974030f2797ba786ba"
   },
   "source": [
    "we see that a simple tuning of parameters can improve xgboost score on GloVe features! Believe me you can squeeze a lot more from it.\n",
    "\n",
    "## Deep Learning\n",
    "\n",
    "But this is an era of deep learning! We cant live without training a few neural networks. Here, we will train LSTM and a simple dense network on the GloVe features. Let's start with the dense network first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "_cell_guid": "ef1aecb1-2cca-48aa-86f5-ef3f27682924",
    "_uuid": "415001df909a6cdfbb23bd18f042ae98207b736c"
   },
   "outputs": [],
   "source": [
    "# scale the data before any neural net:\n",
    "scl = preprocessing.StandardScaler()\n",
    "xtrain_glove_scl = scl.fit_transform(xtrain_glove)\n",
    "xvalid_glove_scl = scl.transform(xvalid_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "_cell_guid": "e2848ff6-79c2-428e-a932-d9b0295f1d48",
    "_uuid": "bee826d9cf5afb4524d00a539870fc3cff9b601a"
   },
   "outputs": [],
   "source": [
    "# we need to binarize the labels for the neural net\n",
    "ytrain_enc = np_utils.to_categorical(ytrain)\n",
    "yvalid_enc = np_utils.to_categorical(yvalid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- keras.np_utils.categorical()을 사용하여 원핫인코딩(One-Hot-Encoding)로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytrain_enc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "_cell_guid": "b544f16b-624d-4b21-998e-aecc94295662",
    "_uuid": "d5294e86eeb398dc97a1ff7f2e09a9d26f5ebe16"
   },
   "outputs": [],
   "source": [
    "# create a simple 3 layer sequential neural net\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(300, input_dim=300, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(300, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dropout: 특정 확률로 node의 activation을 지움. 과적합 방지\n",
    "\n",
    "- 배치 정규화 (BatchNormalization): 경사하강법에서 Gradient Vanishing / Gradient Exploding 이 일어나지 않도록 함, 입력값을 평균 0, 분산 1로 정규화해 네트워크의 학습이 잘 일어나도록 돕는 방식\n",
    "\n",
    "- activation: 'softmax'는 입력받은 값을 출력으로 0~1 사이 값으로 정규화하여 총합은 항상 1이 되도록 하는 함수, 분류하고 싶은 클래스의 수만큼 출력으로 구성되고, 가장 큰 출력 값을 부여받은 클래스가 확률이 가장 높은 것으로 이용된다.\n",
    "\n",
    "- loss: categorical_crossentropy - Softmax activation 뒤에 Cross-Entropy loss를 붙인 형태로 주로 사용하기 때문에 Softmax loss 라고도 불린다. multi-class clssification 문제에서 주로 쓰인다. \n",
    "\n",
    "- optimizer: adam - RMRProp + Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"https://icim.nims.re.kr/file/70679cf085a049679f03736d7afad264.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"https://image.slidesharecdn.com/random-170910154045/95/-49-638.jpg?cb=1505089848\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "_cell_guid": "5c443201-164e-4333-92d2-fea861f77609",
    "_uuid": "b7afdb2c41bb7ffc70a18fc65d403c519193647b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "264/276 [===========================>..] - ETA: 0s - loss: 0.9686"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 892/17621 [52:28<16:24:12,  3.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "276/276 [==============================] - 1s 5ms/step - loss: 0.9628 - val_loss: 0.7687\n",
      "Epoch 2/5\n",
      "276/276 [==============================] - 1s 4ms/step - loss: 0.7499 - val_loss: 0.7318\n",
      "Epoch 3/5\n",
      "276/276 [==============================] - 1s 4ms/step - loss: 0.6872 - val_loss: 0.7180\n",
      "Epoch 4/5\n",
      "276/276 [==============================] - 1s 4ms/step - loss: 0.6533 - val_loss: 0.7248\n",
      "Epoch 5/5\n",
      "276/276 [==============================] - 1s 3ms/step - loss: 0.6170 - val_loss: 0.7112\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f2d995d6190>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(xtrain_glove_scl, y=ytrain_enc, batch_size=64, \n",
    "          epochs=5, verbose=1, \n",
    "          validation_data=(xvalid_glove_scl, yvalid_enc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f8b8a0c2-c086-498c-9663-f83b203af00c",
    "_uuid": "bbbd3b800ad85f9f7b3d964b55812f10323b1262"
   },
   "source": [
    "You need to keep on tuning the parameters of the neural network, add more layers, increase dropout to get better results. Here, I'm just showing that its fast to implement and run and gets better result than xgboost without any optimization :)\n",
    "\n",
    "To move further, i.e. with LSTMs we need to tokenize the text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- rnn은 다른 딥러닝 모델처럼 오차를 줄이기 위해 입력의 반대 방향으로 back propagation을 한다. 이 때 앞에서 들어온 시퀀스 정보를 뒤로 넘겨주변서 비선형 함수, 즉 탄젠트 함수를 지나고 이에 따라 vanishing gradient 문제가 생기게 된다. \n",
    "\n",
    "- 이 문제를 해결하기 위해 나온 것이 LSTM 모델, Long Short Term Memory라는 이름처럼 장기간, 단기간 모두에 적용 가능하다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "_cell_guid": "35e301c1-a17b-476d-a2c3-b606c9088338",
    "_uuid": "9457e2bf1b79db4a7567c1b04755e66b7828d493"
   },
   "outputs": [],
   "source": [
    "# using keras tokenizer here\n",
    "token = text.Tokenizer(num_words=None)\n",
    "max_len = 70\n",
    "\n",
    "token.fit_on_texts(list(xtrain) + list(xvalid))\n",
    "xtrain_seq = token.texts_to_sequences(xtrain)\n",
    "xvalid_seq = token.texts_to_sequences(xvalid)\n",
    "\n",
    "# zero pad the sequences\n",
    "xtrain_pad = sequence.pad_sequences(xtrain_seq, maxlen=max_len)\n",
    "xvalid_pad = sequence.pad_sequences(xvalid_seq, maxlen=max_len)\n",
    "\n",
    "word_index = token.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 토큰화: sentence를 토큰화, num_words에 숫자 입력시 빈도 숫자가 높은 num_words만큼의 단어만을 활용한다.\n",
    "\n",
    "- sequence로 변환: sentences를 Tokenizer를 통해 기계가 알아들을 수 있는 numerical value로 변환한다.\n",
    "\n",
    "- padding: sequences는 길이가 들쭉날쭉하므로, 길이를 맞춰줘야 함. 길이가 긴 문장을 자르거나 짧은 문장은 padding(0으로 채움) 처리해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "_cell_guid": "9cec8fb1-f218-4d65-aa1c-ab3357cb6b22",
    "_uuid": "63ac5c4c795f45474d263a8e21c01333c2234aa6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25943/25943 [00:00<00:00, 296675.42it/s]\n"
     ]
    }
   ],
   "source": [
    "# create an embedding matrix for the words we have in the dataset\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "for word, i in tqdm(word_index.items()):\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전체 단어를 300차원의 Vector로 임베딩하고 싶은 경우엔 단어 개수 x 300의 Embedding Matrix가 필요함\n",
    "\n",
    "1 x 단어 개수와 단어 개수 x 300를 행렬곱하면 자신의 Index로 인덱싱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "_cell_guid": "7f2fa769-d70e-43ef-b6ce-3b82fccaef32",
    "_uuid": "f978f21365eb8bf6684a70e3ae6fae2e8f4fff1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
     ]
    }
   ],
   "source": [
    "# A simple LSTM with glove embeddings and two dense layers\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                     300,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=False))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(LSTM(100, dropout=0.3, recurrent_dropout=0.3))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SpatialDropout1D:  특정 컬럼을 전체를 떨구는것으로써, 텍스트분석등은 대충 떨궈놓으면 어차피 주위 단어로 유추가 전부 가능하기 때문에 드랍아웃을 하는 의의가 없어지기 때문에 한줄을 죄다 없애버린다.\n",
    "\n",
    "- activation: CIFAR에서 지원한 hinton 교수가 2006년에 Neural Network가 발전하지 못했던 이유를 연구하다가 새로운 방법을 찾아냈다. 그 동안은 non-linearity에 대해 잘못된 방법을 사용했다는 것이었다. 여기서 non-linearity는 sigmoid 함수를 말한다. vanishing gradient 문제의 발생 원인은 sigmoid 함수에 있었다. sigmoid 함수가 값을 변형하면서 이런 문제가 생긴 것이었다. ReLU 함수는 그림에 있는 것처럼 0보다 작을 때는 0을 사용하고, 0보다 큰 값에 대해서는 해당 값을 그대로 사용하는 방법이다. 음수에 대해서는 값이 바뀌지만, 양수에 대해서는 값을 바꾸지 않는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"https://t1.daumcdn.net/cfile/tistory/22293C50579F7BBF13\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "_cell_guid": "67593280-81e1-44eb-9cd1-198822f316e8",
    "_uuid": "73d8a4dc4d0e245e979d7d142d1b5c7a4057aa7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "35/35 [==============================] - 9s 266ms/step - loss: 1.0647 - val_loss: 0.9315\n",
      "Epoch 2/100\n",
      "35/35 [==============================] - 9s 256ms/step - loss: 0.9106 - val_loss: 0.7853\n",
      "Epoch 3/100\n",
      "35/35 [==============================] - 9s 266ms/step - loss: 0.8456 - val_loss: 0.7538\n",
      "Epoch 4/100\n",
      "35/35 [==============================] - 9s 259ms/step - loss: 0.8124 - val_loss: 0.7350\n",
      "Epoch 5/100\n",
      "35/35 [==============================] - 9s 259ms/step - loss: 0.7942 - val_loss: 0.7107\n",
      "Epoch 6/100\n",
      "35/35 [==============================] - 9s 262ms/step - loss: 0.7706 - val_loss: 0.7074\n",
      "Epoch 7/100\n",
      "35/35 [==============================] - 9s 263ms/step - loss: 0.7568 - val_loss: 0.6873\n",
      "Epoch 8/100\n",
      "35/35 [==============================] - 9s 259ms/step - loss: 0.7378 - val_loss: 0.6889\n",
      "Epoch 9/100\n",
      "35/35 [==============================] - 9s 258ms/step - loss: 0.7278 - val_loss: 0.6707\n",
      "Epoch 10/100\n",
      "35/35 [==============================] - 9s 255ms/step - loss: 0.7063 - val_loss: 0.6421\n",
      "Epoch 11/100\n",
      "35/35 [==============================] - 9s 257ms/step - loss: 0.6867 - val_loss: 0.6557\n",
      "Epoch 12/100\n",
      "35/35 [==============================] - 9s 248ms/step - loss: 0.6791 - val_loss: 0.6480\n",
      "Epoch 13/100\n",
      "35/35 [==============================] - 9s 255ms/step - loss: 0.6590 - val_loss: 0.6197\n",
      "Epoch 14/100\n",
      "35/35 [==============================] - 9s 255ms/step - loss: 0.6437 - val_loss: 0.6194\n",
      "Epoch 15/100\n",
      "35/35 [==============================] - 9s 256ms/step - loss: 0.6249 - val_loss: 0.6608\n",
      "Epoch 16/100\n",
      "35/35 [==============================] - 9s 262ms/step - loss: 0.6135 - val_loss: 0.5960\n",
      "Epoch 17/100\n",
      "35/35 [==============================] - 9s 257ms/step - loss: 0.6021 - val_loss: 0.6365\n",
      "Epoch 18/100\n",
      "35/35 [==============================] - 10s 272ms/step - loss: 0.5994 - val_loss: 0.5882\n",
      "Epoch 19/100\n",
      "35/35 [==============================] - 9s 255ms/step - loss: 0.5735 - val_loss: 0.5948\n",
      "Epoch 20/100\n",
      "35/35 [==============================] - 9s 253ms/step - loss: 0.5653 - val_loss: 0.5882\n",
      "Epoch 21/100\n",
      "35/35 [==============================] - 9s 259ms/step - loss: 0.5553 - val_loss: 0.5825\n",
      "Epoch 22/100\n",
      "35/35 [==============================] - 9s 262ms/step - loss: 0.5504 - val_loss: 0.5901\n",
      "Epoch 23/100\n",
      "35/35 [==============================] - 9s 258ms/step - loss: 0.5432 - val_loss: 0.5965\n",
      "Epoch 24/100\n",
      "35/35 [==============================] - 9s 259ms/step - loss: 0.5289 - val_loss: 0.5860\n",
      "Epoch 25/100\n",
      "35/35 [==============================] - 9s 260ms/step - loss: 0.5141 - val_loss: 0.5847\n",
      "Epoch 26/100\n",
      "35/35 [==============================] - 9s 259ms/step - loss: 0.5126 - val_loss: 0.5776\n",
      "Epoch 27/100\n",
      "35/35 [==============================] - 9s 257ms/step - loss: 0.4953 - val_loss: 0.5875\n",
      "Epoch 28/100\n",
      "35/35 [==============================] - 9s 258ms/step - loss: 0.4860 - val_loss: 0.5817\n",
      "Epoch 29/100\n",
      "35/35 [==============================] - 9s 257ms/step - loss: 0.4763 - val_loss: 0.5926\n",
      "Epoch 30/100\n",
      "35/35 [==============================] - 9s 266ms/step - loss: 0.4753 - val_loss: 0.5685\n",
      "Epoch 31/100\n",
      "35/35 [==============================] - 9s 249ms/step - loss: 0.4731 - val_loss: 0.5711\n",
      "Epoch 32/100\n",
      "35/35 [==============================] - 9s 266ms/step - loss: 0.4571 - val_loss: 0.5750\n",
      "Epoch 33/100\n",
      "35/35 [==============================] - 9s 259ms/step - loss: 0.4589 - val_loss: 0.5676\n",
      "Epoch 34/100\n",
      "35/35 [==============================] - 9s 262ms/step - loss: 0.4399 - val_loss: 0.5806\n",
      "Epoch 35/100\n",
      "35/35 [==============================] - 9s 257ms/step - loss: 0.4370 - val_loss: 0.5806\n",
      "Epoch 36/100\n",
      "35/35 [==============================] - 9s 258ms/step - loss: 0.4290 - val_loss: 0.5821\n",
      "Epoch 37/100\n",
      "35/35 [==============================] - 9s 256ms/step - loss: 0.4226 - val_loss: 0.5914\n",
      "Epoch 38/100\n",
      "35/35 [==============================] - 9s 259ms/step - loss: 0.4264 - val_loss: 0.5798\n",
      "Epoch 39/100\n",
      "35/35 [==============================] - 9s 255ms/step - loss: 0.4153 - val_loss: 0.5651\n",
      "Epoch 40/100\n",
      "35/35 [==============================] - 9s 266ms/step - loss: 0.4066 - val_loss: 0.5944\n",
      "Epoch 41/100\n",
      "35/35 [==============================] - 9s 254ms/step - loss: 0.4075 - val_loss: 0.5839\n",
      "Epoch 42/100\n",
      "35/35 [==============================] - 9s 260ms/step - loss: 0.4004 - val_loss: 0.5812\n",
      "Epoch 43/100\n",
      "35/35 [==============================] - 9s 258ms/step - loss: 0.4008 - val_loss: 0.6134\n",
      "Epoch 44/100\n",
      "35/35 [==============================] - 9s 262ms/step - loss: 0.3873 - val_loss: 0.5955\n",
      "Epoch 45/100\n",
      "35/35 [==============================] - 9s 258ms/step - loss: 0.3819 - val_loss: 0.6134\n",
      "Epoch 46/100\n",
      "35/35 [==============================] - 9s 271ms/step - loss: 0.3806 - val_loss: 0.5949\n",
      "Epoch 47/100\n",
      "35/35 [==============================] - 9s 257ms/step - loss: 0.3792 - val_loss: 0.6157\n",
      "Epoch 48/100\n",
      "35/35 [==============================] - 9s 257ms/step - loss: 0.3653 - val_loss: 0.6196\n",
      "Epoch 49/100\n",
      "35/35 [==============================] - 9s 252ms/step - loss: 0.3711 - val_loss: 0.6158\n",
      "Epoch 50/100\n",
      "35/35 [==============================] - 9s 262ms/step - loss: 0.3742 - val_loss: 0.6068\n",
      "Epoch 51/100\n",
      "35/35 [==============================] - 9s 248ms/step - loss: 0.3654 - val_loss: 0.6068\n",
      "Epoch 52/100\n",
      "35/35 [==============================] - 9s 256ms/step - loss: 0.3545 - val_loss: 0.6332\n",
      "Epoch 53/100\n",
      "35/35 [==============================] - 9s 253ms/step - loss: 0.3519 - val_loss: 0.6205\n",
      "Epoch 54/100\n",
      "35/35 [==============================] - 9s 260ms/step - loss: 0.3513 - val_loss: 0.6277\n",
      "Epoch 55/100\n",
      "35/35 [==============================] - 9s 254ms/step - loss: 0.3502 - val_loss: 0.6099\n",
      "Epoch 56/100\n",
      "35/35 [==============================] - 9s 256ms/step - loss: 0.3397 - val_loss: 0.6603\n",
      "Epoch 57/100\n",
      "35/35 [==============================] - 9s 261ms/step - loss: 0.3304 - val_loss: 0.6061\n",
      "Epoch 58/100\n",
      "35/35 [==============================] - 9s 270ms/step - loss: 0.3367 - val_loss: 0.6381\n",
      "Epoch 59/100\n",
      "35/35 [==============================] - 9s 258ms/step - loss: 0.3184 - val_loss: 0.6400\n",
      "Epoch 60/100\n",
      "35/35 [==============================] - 9s 258ms/step - loss: 0.3213 - val_loss: 0.6590\n",
      "Epoch 61/100\n",
      "35/35 [==============================] - 9s 256ms/step - loss: 0.3233 - val_loss: 0.6269\n",
      "Epoch 62/100\n",
      "35/35 [==============================] - 9s 260ms/step - loss: 0.3210 - val_loss: 0.6264\n",
      "Epoch 63/100\n",
      "35/35 [==============================] - 9s 253ms/step - loss: 0.3181 - val_loss: 0.6275\n",
      "Epoch 64/100\n",
      "35/35 [==============================] - 9s 254ms/step - loss: 0.3149 - val_loss: 0.6460\n",
      "Epoch 65/100\n",
      "35/35 [==============================] - 9s 261ms/step - loss: 0.3110 - val_loss: 0.6204\n",
      "Epoch 66/100\n",
      "35/35 [==============================] - 9s 260ms/step - loss: 0.3156 - val_loss: 0.6508\n",
      "Epoch 67/100\n",
      "35/35 [==============================] - 10s 273ms/step - loss: 0.3007 - val_loss: 0.6368\n",
      "Epoch 68/100\n",
      "35/35 [==============================] - 9s 258ms/step - loss: 0.2986 - val_loss: 0.6407\n",
      "Epoch 69/100\n",
      "35/35 [==============================] - 9s 259ms/step - loss: 0.3070 - val_loss: 0.6317\n",
      "Epoch 70/100\n",
      "35/35 [==============================] - 9s 254ms/step - loss: 0.3024 - val_loss: 0.6465\n",
      "Epoch 71/100\n",
      "35/35 [==============================] - 9s 265ms/step - loss: 0.2962 - val_loss: 0.6676\n",
      "Epoch 72/100\n",
      "35/35 [==============================] - 9s 255ms/step - loss: 0.2930 - val_loss: 0.6493\n",
      "Epoch 73/100\n",
      "35/35 [==============================] - 9s 253ms/step - loss: 0.2978 - val_loss: 0.6441\n",
      "Epoch 74/100\n",
      "35/35 [==============================] - 9s 263ms/step - loss: 0.2951 - val_loss: 0.6990\n",
      "Epoch 75/100\n",
      "35/35 [==============================] - 9s 261ms/step - loss: 0.2856 - val_loss: 0.6447\n",
      "Epoch 76/100\n",
      "35/35 [==============================] - 9s 259ms/step - loss: 0.2810 - val_loss: 0.7297\n",
      "Epoch 77/100\n",
      "35/35 [==============================] - 9s 249ms/step - loss: 0.2792 - val_loss: 0.6968\n",
      "Epoch 78/100\n",
      "35/35 [==============================] - 9s 256ms/step - loss: 0.2797 - val_loss: 0.7132\n",
      "Epoch 79/100\n",
      "35/35 [==============================] - 9s 266ms/step - loss: 0.2840 - val_loss: 0.6586\n",
      "Epoch 80/100\n",
      "35/35 [==============================] - 9s 261ms/step - loss: 0.2755 - val_loss: 0.6591\n",
      "Epoch 81/100\n",
      "35/35 [==============================] - 9s 259ms/step - loss: 0.2728 - val_loss: 0.7178\n",
      "Epoch 82/100\n",
      "35/35 [==============================] - 9s 260ms/step - loss: 0.2758 - val_loss: 0.7197\n",
      "Epoch 83/100\n",
      "35/35 [==============================] - 9s 264ms/step - loss: 0.2760 - val_loss: 0.7014\n",
      "Epoch 84/100\n",
      "35/35 [==============================] - 9s 247ms/step - loss: 0.2672 - val_loss: 0.7304\n",
      "Epoch 85/100\n",
      "35/35 [==============================] - 9s 251ms/step - loss: 0.2610 - val_loss: 0.6852\n",
      "Epoch 86/100\n",
      "35/35 [==============================] - 9s 265ms/step - loss: 0.2709 - val_loss: 0.6590\n",
      "Epoch 87/100\n",
      "35/35 [==============================] - 9s 270ms/step - loss: 0.2561 - val_loss: 0.6894\n",
      "Epoch 88/100\n",
      "35/35 [==============================] - 9s 258ms/step - loss: 0.2554 - val_loss: 0.6661\n",
      "Epoch 89/100\n",
      "35/35 [==============================] - 9s 255ms/step - loss: 0.2588 - val_loss: 0.6796\n",
      "Epoch 90/100\n",
      "35/35 [==============================] - 9s 261ms/step - loss: 0.2507 - val_loss: 0.7132\n",
      "Epoch 91/100\n",
      "35/35 [==============================] - 9s 255ms/step - loss: 0.2462 - val_loss: 0.6981\n",
      "Epoch 92/100\n",
      "35/35 [==============================] - 9s 262ms/step - loss: 0.2557 - val_loss: 0.6787\n",
      "Epoch 93/100\n",
      "35/35 [==============================] - 10s 274ms/step - loss: 0.2525 - val_loss: 0.6789\n",
      "Epoch 94/100\n",
      "35/35 [==============================] - 9s 259ms/step - loss: 0.2482 - val_loss: 0.7301\n",
      "Epoch 95/100\n",
      "35/35 [==============================] - 9s 257ms/step - loss: 0.2432 - val_loss: 0.7102\n",
      "Epoch 96/100\n",
      "35/35 [==============================] - 9s 259ms/step - loss: 0.2494 - val_loss: 0.7028\n",
      "Epoch 97/100\n",
      "35/35 [==============================] - 9s 258ms/step - loss: 0.2500 - val_loss: 0.7343\n",
      "Epoch 98/100\n",
      "35/35 [==============================] - 9s 250ms/step - loss: 0.2430 - val_loss: 0.7246\n",
      "Epoch 99/100\n",
      "35/35 [==============================] - 9s 251ms/step - loss: 0.2316 - val_loss: 0.7504\n",
      "Epoch 100/100\n",
      "35/35 [==============================] - 9s 252ms/step - loss: 0.2408 - val_loss: 0.7032\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f2ddf9d4610>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs=100, verbose=1, validation_data=(xvalid_pad, yvalid_enc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d84364cd-85de-4106-91e9-7eca06dab718",
    "_uuid": "01aa520c9ef6283a5399d2dd725d917c67d10213"
   },
   "source": [
    "We see that the score is now less than 0.5. I ran it for many epochs without stopping at the best but you can use early stopping to stop at the best iteration. How do I use early stopping?\n",
    "\n",
    "well, pretty easy. let's compile the model again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "_cell_guid": "953a3e6f-0052-4f71-9697-4c3a64846e17",
    "_uuid": "800379d15f6173755b1ec9b2e2d5b5a04509a1b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Epoch 1/100\n",
      "35/35 [==============================] - 9s 266ms/step - loss: 1.0340 - val_loss: 0.8331\n",
      "Epoch 2/100\n",
      "35/35 [==============================] - 9s 264ms/step - loss: 0.8837 - val_loss: 0.7624\n",
      "Epoch 3/100\n",
      "35/35 [==============================] - 9s 255ms/step - loss: 0.8447 - val_loss: 0.7550\n",
      "Epoch 4/100\n",
      "35/35 [==============================] - 9s 247ms/step - loss: 0.8185 - val_loss: 0.7486\n",
      "Epoch 5/100\n",
      "35/35 [==============================] - 9s 245ms/step - loss: 0.7929 - val_loss: 0.7234\n",
      "Epoch 6/100\n",
      "35/35 [==============================] - 8s 236ms/step - loss: 0.7650 - val_loss: 0.6992\n",
      "Epoch 7/100\n",
      "35/35 [==============================] - 9s 251ms/step - loss: 0.7514 - val_loss: 0.6884\n",
      "Epoch 8/100\n",
      "35/35 [==============================] - 9s 250ms/step - loss: 0.7380 - val_loss: 0.6898\n",
      "Epoch 9/100\n",
      "35/35 [==============================] - 9s 260ms/step - loss: 0.7250 - val_loss: 0.6991\n",
      "Epoch 10/100\n",
      "35/35 [==============================] - 8s 240ms/step - loss: 0.6979 - val_loss: 0.6414\n",
      "Epoch 11/100\n",
      "35/35 [==============================] - 9s 252ms/step - loss: 0.6679 - val_loss: 0.6311\n",
      "Epoch 12/100\n",
      "35/35 [==============================] - 9s 247ms/step - loss: 0.6549 - val_loss: 0.6593\n",
      "Epoch 13/100\n",
      "35/35 [==============================] - 9s 255ms/step - loss: 0.6344 - val_loss: 0.6281\n",
      "Epoch 14/100\n",
      "35/35 [==============================] - 9s 250ms/step - loss: 0.6156 - val_loss: 0.5944\n",
      "Epoch 15/100\n",
      "35/35 [==============================] - 8s 242ms/step - loss: 0.5908 - val_loss: 0.6282\n",
      "Epoch 16/100\n",
      "35/35 [==============================] - 9s 248ms/step - loss: 0.5909 - val_loss: 0.6395\n",
      "Epoch 17/100\n",
      "35/35 [==============================] - 9s 258ms/step - loss: 0.5753 - val_loss: 0.6081\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f2ddf394340>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A simple LSTM with glove embeddings and two dense layers\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                     300,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=False))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(LSTM(300, dropout=0.3, recurrent_dropout=0.3))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# Fit the model with early stopping callback\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "model.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs=100, \n",
    "          verbose=1, validation_data=(xvalid_pad, yvalid_enc), callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- earlystop \n",
    "\n",
    "- mode : mode 의 default는 auto 인데, 이는 keras 에서 알아서 min, max 여부를 선택하게 된다. performance measure를 정의하고, 이것을 최대화 할지, 최소화 할지를 지정하는 것이다. 그러면 keras 에서 알아서 적절한 epoch 에서 training 을 멈춘다. \n",
    "   - auto : 관찰하는 이름에 따라 자동으로 지정\n",
    "   - min : 관찰하고 있는 항목이 감소되는 것을 멈출 때 종료\n",
    "   - max : 관찰하고 있는 항목이 증가되는 것을 멈출 때 종료\n",
    "\n",
    "\n",
    "- min_delta: 개선되고 있다고 판단하기 위한 최소 변화량을 나타낸다. 만약 변화량이 min_delta보다 적은 경우에는 개선이 없다고 판단한다.\n",
    "\n",
    "- verbose=1 로 지정하면, 언제 keras 에서 training 을 멈추었는지를 화면에 출력할 수 있다.\n",
    "\n",
    "- patience: 성능이 증가하지 않는다고, 그 순간 바로 멈추는 것은 효과적이지않을 수 있다. patience 는 성능이 증가하지 않는 epoch 을 몇 번이나 허용할 것인가를 정의한다. partience 는 다소 주관적인 기준이다. 사용한 데이터와 모델의 설계에 따라 최적의 값이 바뀔 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "bf4c30e8-de2f-4787-b5ac-07a22089518e",
    "_uuid": "c2a05818abc8c70e3a8c4ab7118d3815a0b07147"
   },
   "source": [
    "One question could be: why do i use so much dropout? Well, fit the model with no or little dropout and you will that it starts to overfit :)\n",
    "\n",
    "Let's see if Bi-directional LSTM can give us better results. Its a piece of cake to do it with Keras :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- LSTM은 이전 step이 이후 step에 영향을 주는 것은 고려하지만, 이후 step이 이전 step에 영향을 주는 것은 고려하지 못한다. 따라서 텍스트 데이터는 정방향(시점을 기준으로 과거에서 미래 방향) 추론과 역방향(시점을 기준으로 미래에서 과거 방향) 추론을 모두 고려할 때 비로소 유의미한 결과를 낼 수 있다. 이에 Bi LSTM은 forward와 backward를 병합해 사용하는 방식을 선택하고 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "_cell_guid": "51093329-081b-48d7-b29c-28a0b99f038f",
    "_uuid": "be091fd7888ef0239d7c76536e6a430ca3edca88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Epoch 1/100\n",
      "35/35 [==============================] - 17s 484ms/step - loss: 1.0450 - val_loss: 0.8760\n",
      "Epoch 2/100\n",
      "35/35 [==============================] - 17s 492ms/step - loss: 0.8808 - val_loss: 0.7824\n",
      "Epoch 3/100\n",
      "35/35 [==============================] - 17s 478ms/step - loss: 0.8370 - val_loss: 0.7438\n",
      "Epoch 4/100\n",
      "35/35 [==============================] - 17s 484ms/step - loss: 0.8067 - val_loss: 0.7452\n",
      "Epoch 5/100\n",
      "35/35 [==============================] - 17s 483ms/step - loss: 0.7876 - val_loss: 0.7189\n",
      "Epoch 6/100\n",
      "35/35 [==============================] - 17s 489ms/step - loss: 0.7645 - val_loss: 0.7127\n",
      "Epoch 7/100\n",
      "35/35 [==============================] - 17s 488ms/step - loss: 0.7535 - val_loss: 0.6970\n",
      "Epoch 8/100\n",
      "35/35 [==============================] - 16s 471ms/step - loss: 0.7268 - val_loss: 0.6833\n",
      "Epoch 9/100\n",
      "35/35 [==============================] - 17s 483ms/step - loss: 0.7041 - val_loss: 0.6867\n",
      "Epoch 10/100\n",
      "35/35 [==============================] - 17s 487ms/step - loss: 0.6801 - val_loss: 0.6341\n",
      "Epoch 11/100\n",
      "35/35 [==============================] - 17s 475ms/step - loss: 0.6651 - val_loss: 0.6721\n",
      "Epoch 12/100\n",
      "35/35 [==============================] - 17s 475ms/step - loss: 0.6486 - val_loss: 0.6253\n",
      "Epoch 13/100\n",
      "35/35 [==============================] - 17s 489ms/step - loss: 0.6205 - val_loss: 0.6584\n",
      "Epoch 14/100\n",
      "35/35 [==============================] - 17s 485ms/step - loss: 0.6103 - val_loss: 0.6102\n",
      "Epoch 15/100\n",
      "35/35 [==============================] - 16s 466ms/step - loss: 0.5729 - val_loss: 0.6114\n",
      "Epoch 16/100\n",
      "35/35 [==============================] - 17s 489ms/step - loss: 0.5564 - val_loss: 0.5903\n",
      "Epoch 17/100\n",
      "35/35 [==============================] - 17s 473ms/step - loss: 0.5425 - val_loss: 0.5912\n",
      "Epoch 18/100\n",
      "35/35 [==============================] - 17s 488ms/step - loss: 0.5433 - val_loss: 0.5806\n",
      "Epoch 19/100\n",
      "35/35 [==============================] - 17s 480ms/step - loss: 0.5167 - val_loss: 0.5831\n",
      "Epoch 20/100\n",
      "35/35 [==============================] - 18s 500ms/step - loss: 0.4969 - val_loss: 0.5950\n",
      "Epoch 21/100\n",
      "35/35 [==============================] - 17s 489ms/step - loss: 0.4868 - val_loss: 0.5939\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f2d8ce1db20>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A simple bidirectional LSTM with glove embeddings and two dense layers\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                     300,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=False))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(Bidirectional(LSTM(300, dropout=0.3, recurrent_dropout=0.3)))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# Fit the model with early stopping callback\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "model.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs=100, \n",
    "          verbose=1, validation_data=(xvalid_pad, yvalid_enc), callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "69875648-b716-457a-8db8-bcf625d7d3bf",
    "_uuid": "a1019cf81782debe55d16d55d90b4e1bc3076176"
   },
   "source": [
    "Pretty close! Lets try two layers of GRU:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- GRU(Gated Recurrent Unit)는 2014년 뉴욕대학교 조경현 교수님이 집필한 논문에서 제안되었다. GRU는 LSTM의 장기 의존성 문제에 대한 해결책을 유지하면서, 은닉 상태를 업데이트하는 계산을 줄였다. 다시 말해서, GRU는 성능은 LSTM과 유사하면서 복잡했던 LSTM의 구조를 간단화 시킨 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "_cell_guid": "e427650c-11e9-422b-9e06-45e353721857",
    "_uuid": "5196303d6875a3d1cd9432631a6a0ecfe0be4d3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer gru will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer gru_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Epoch 1/100\n",
      "35/35 [==============================] - 18s 507ms/step - loss: 1.0657 - val_loss: 0.9584\n",
      "Epoch 2/100\n",
      "35/35 [==============================] - 16s 469ms/step - loss: 0.9325 - val_loss: 0.9260\n",
      "Epoch 3/100\n",
      "35/35 [==============================] - 17s 474ms/step - loss: 0.8702 - val_loss: 0.8150\n",
      "Epoch 4/100\n",
      "35/35 [==============================] - 16s 470ms/step - loss: 0.8301 - val_loss: 0.7615\n",
      "Epoch 5/100\n",
      "35/35 [==============================] - 16s 470ms/step - loss: 0.8051 - val_loss: 0.7486\n",
      "Epoch 6/100\n",
      "35/35 [==============================] - 16s 460ms/step - loss: 0.7797 - val_loss: 0.7001\n",
      "Epoch 7/100\n",
      "35/35 [==============================] - 16s 467ms/step - loss: 0.7534 - val_loss: 0.6768\n",
      "Epoch 8/100\n",
      "35/35 [==============================] - 16s 464ms/step - loss: 0.7308 - val_loss: 0.6859\n",
      "Epoch 9/100\n",
      "35/35 [==============================] - 17s 480ms/step - loss: 0.7077 - val_loss: 0.6841\n",
      "Epoch 10/100\n",
      "35/35 [==============================] - 17s 472ms/step - loss: 0.7002 - val_loss: 0.6505\n",
      "Epoch 11/100\n",
      "35/35 [==============================] - 16s 456ms/step - loss: 0.6715 - val_loss: 0.6422\n",
      "Epoch 12/100\n",
      "35/35 [==============================] - 16s 465ms/step - loss: 0.6523 - val_loss: 0.6155\n",
      "Epoch 13/100\n",
      "35/35 [==============================] - 17s 472ms/step - loss: 0.6389 - val_loss: 0.6283\n",
      "Epoch 14/100\n",
      "35/35 [==============================] - 17s 474ms/step - loss: 0.6117 - val_loss: 0.6239\n",
      "Epoch 15/100\n",
      "35/35 [==============================] - 16s 463ms/step - loss: 0.6033 - val_loss: 0.5880\n",
      "Epoch 16/100\n",
      "35/35 [==============================] - 16s 460ms/step - loss: 0.5877 - val_loss: 0.5828\n",
      "Epoch 17/100\n",
      "35/35 [==============================] - 16s 458ms/step - loss: 0.5716 - val_loss: 0.5913\n",
      "Epoch 18/100\n",
      "35/35 [==============================] - 17s 474ms/step - loss: 0.5600 - val_loss: 0.6043\n",
      "Epoch 19/100\n",
      "35/35 [==============================] - 17s 476ms/step - loss: 0.5508 - val_loss: 0.5724\n",
      "Epoch 20/100\n",
      "35/35 [==============================] - 17s 481ms/step - loss: 0.5389 - val_loss: 0.5729\n",
      "Epoch 21/100\n",
      "35/35 [==============================] - 16s 463ms/step - loss: 0.5141 - val_loss: 0.5581\n",
      "Epoch 22/100\n",
      "35/35 [==============================] - 16s 469ms/step - loss: 0.5146 - val_loss: 0.5695\n",
      "Epoch 23/100\n",
      "35/35 [==============================] - 17s 479ms/step - loss: 0.4946 - val_loss: 0.5521\n",
      "Epoch 24/100\n",
      "35/35 [==============================] - 16s 461ms/step - loss: 0.4866 - val_loss: 0.5491\n",
      "Epoch 25/100\n",
      "35/35 [==============================] - 17s 473ms/step - loss: 0.4659 - val_loss: 0.5519\n",
      "Epoch 26/100\n",
      "35/35 [==============================] - 16s 451ms/step - loss: 0.4540 - val_loss: 0.5424\n",
      "Epoch 27/100\n",
      "35/35 [==============================] - 16s 464ms/step - loss: 0.4647 - val_loss: 0.5579\n",
      "Epoch 28/100\n",
      "35/35 [==============================] - 16s 470ms/step - loss: 0.4448 - val_loss: 0.5606\n",
      "Epoch 29/100\n",
      "35/35 [==============================] - 17s 479ms/step - loss: 0.4149 - val_loss: 0.5517\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f2d204f20a0>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GRU with glove embeddings and two dense layers\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                     300,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=False))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(GRU(300, dropout=0.3, recurrent_dropout=0.3, return_sequences=True))\n",
    "model.add(GRU(300, dropout=0.3, recurrent_dropout=0.3))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# Fit the model with early stopping callback\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "model.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs=100, \n",
    "          verbose=1, validation_data=(xvalid_pad, yvalid_enc), callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "aeb72e91-0b0c-449e-a41b-5d2c1e6a6f26",
    "_uuid": "dfcac382d1c84f19ca12fa3a590ef2c67a30af75"
   },
   "source": [
    "Nice! Much better than what we had previously! Keep optimizing and the performance will keep improving.\n",
    "Worth trying: stemming and lemmatization. This is something I'm skipping for now.\n",
    "\n",
    "In the Kaggle world, to get a top score you should have an ensemble of models. Let's check a little bit of ensembling!\n",
    "\n",
    "\n",
    "## Ensembling\n",
    "\n",
    "Few months back I made a simple ensembler but I didn't have time to develop it fully. It can be found here: https://github.com/abhishekkrthakur/pysembler . I'm going to use some part of it here:\n",
    "\n",
    "- 앙상블 학습은 여러 개의 모델을 결합하여 하나의 모델보다 더 좋은 성능을 내는 머신러닝 기법이다. 앙상블 학습의 핵심은 여러 개의 약 분류기 (Weak Classifier)를 결합하여 강 분류기(Strong Classifier)를 만드는 것이다. 그리하여 모델의 정확성이 향상된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "_cell_guid": "074b6b36-c419-4be4-b419-66e00b247f5e",
    "_uuid": "051f7ed5cbace1a16ad6a355bc50293ce13ee5f4"
   },
   "outputs": [],
   "source": [
    "# this is the main ensembling class. how to use it is in the next cell!\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format=\"[%(asctime)s] %(levelname)s %(message)s\",\n",
    "    datefmt=\"%H:%M:%S\", stream=sys.stdout)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class Ensembler(object):\n",
    "    def __init__(self, model_dict, num_folds=3, task_type='classification', optimize=roc_auc_score,\n",
    "                 lower_is_better=False, save_path=None):\n",
    "        \"\"\"\n",
    "        Ensembler init function\n",
    "        :param model_dict: model dictionary, see README for its format\n",
    "        :param num_folds: the number of folds for ensembling\n",
    "        :param task_type: classification or regression\n",
    "        :param optimize: the function to optimize for, e.g. AUC, logloss, etc. Must have two arguments y_test and y_pred\n",
    "        :param lower_is_better: is lower value of optimization function better or higher\n",
    "        :param save_path: path to which model pickles will be dumped to along with generated predictions, or None\n",
    "        \"\"\"\n",
    "\n",
    "        self.model_dict = model_dict\n",
    "        self.levels = len(self.model_dict)\n",
    "        self.num_folds = num_folds\n",
    "        self.task_type = task_type\n",
    "        self.optimize = optimize\n",
    "        self.lower_is_better = lower_is_better\n",
    "        self.save_path = save_path\n",
    "\n",
    "        self.training_data = None\n",
    "        self.test_data = None\n",
    "        self.y = None\n",
    "        self.lbl_enc = None\n",
    "        self.y_enc = None\n",
    "        self.train_prediction_dict = None\n",
    "        self.test_prediction_dict = None\n",
    "        self.num_classes = None\n",
    "\n",
    "    def fit(self, training_data, y, lentrain):\n",
    "        \"\"\"\n",
    "        :param training_data: training data in tabular format\n",
    "        :param y: binary, multi-class or regression\n",
    "        :return: chain of models to be used in prediction\n",
    "        \"\"\"\n",
    "\n",
    "        self.training_data = training_data\n",
    "        self.y = y\n",
    "\n",
    "        if self.task_type == 'classification':\n",
    "            self.num_classes = len(np.unique(self.y))\n",
    "            logger.info(\"Found %d classes\", self.num_classes)\n",
    "            self.lbl_enc = LabelEncoder()\n",
    "            self.y_enc = self.lbl_enc.fit_transform(self.y)\n",
    "            kf = StratifiedKFold(n_splits=self.num_folds)\n",
    "            train_prediction_shape = (lentrain, self.num_classes)\n",
    "        else:\n",
    "            self.num_classes = -1\n",
    "            self.y_enc = self.y\n",
    "            kf = KFold(n_splits=self.num_folds)\n",
    "            train_prediction_shape = (lentrain, 1)\n",
    "\n",
    "        self.train_prediction_dict = {}\n",
    "        for level in range(self.levels):\n",
    "            self.train_prediction_dict[level] = np.zeros((train_prediction_shape[0],\n",
    "                                                          train_prediction_shape[1] * len(self.model_dict[level])))\n",
    "\n",
    "        for level in range(self.levels):\n",
    "\n",
    "            if level == 0:\n",
    "                temp_train = self.training_data\n",
    "            else:\n",
    "                temp_train = self.train_prediction_dict[level - 1]\n",
    "\n",
    "            for model_num, model in enumerate(self.model_dict[level]):\n",
    "                validation_scores = []\n",
    "                foldnum = 1\n",
    "                for train_index, valid_index in kf.split(self.train_prediction_dict[0], self.y_enc):\n",
    "                    logger.info(\"Training Level %d Fold # %d. Model # %d\", level, foldnum, model_num)\n",
    "\n",
    "                    if level != 0:\n",
    "                        l_training_data = temp_train[train_index]\n",
    "                        l_validation_data = temp_train[valid_index]\n",
    "                        model.fit(l_training_data, self.y_enc[train_index])\n",
    "                    else:\n",
    "                        l0_training_data = temp_train[0][model_num]\n",
    "                        if type(l0_training_data) == list:\n",
    "                            l_training_data = [x[train_index] for x in l0_training_data]\n",
    "                            l_validation_data = [x[valid_index] for x in l0_training_data]\n",
    "                        else:\n",
    "                            l_training_data = l0_training_data[train_index]\n",
    "                            l_validation_data = l0_training_data[valid_index]\n",
    "                        model.fit(l_training_data, self.y_enc[train_index])\n",
    "\n",
    "                    logger.info(\"Predicting Level %d. Fold # %d. Model # %d\", level, foldnum, model_num)\n",
    "\n",
    "                    if self.task_type == 'classification':\n",
    "                        temp_train_predictions = model.predict_proba(l_validation_data)\n",
    "                        self.train_prediction_dict[level][valid_index,\n",
    "                        (model_num * self.num_classes):(model_num * self.num_classes) +\n",
    "                                                       self.num_classes] = temp_train_predictions\n",
    "\n",
    "                    else:\n",
    "                        temp_train_predictions = model.predict(l_validation_data)\n",
    "                        self.train_prediction_dict[level][valid_index, model_num] = temp_train_predictions\n",
    "                    validation_score = self.optimize(self.y_enc[valid_index], temp_train_predictions)\n",
    "                    validation_scores.append(validation_score)\n",
    "                    logger.info(\"Level %d. Fold # %d. Model # %d. Validation Score = %f\", level, foldnum, model_num,\n",
    "                                validation_score)\n",
    "                    foldnum += 1\n",
    "                avg_score = np.mean(validation_scores)\n",
    "                std_score = np.std(validation_scores)\n",
    "                logger.info(\"Level %d. Model # %d. Mean Score = %f. Std Dev = %f\", level, model_num,\n",
    "                            avg_score, std_score)\n",
    "\n",
    "            logger.info(\"Saving predictions for level # %d\", level)\n",
    "            train_predictions_df = pd.DataFrame(self.train_prediction_dict[level])\n",
    "            train_predictions_df.to_csv(os.path.join(self.save_path, \"train_predictions_level_\" + str(level) + \".csv\"),\n",
    "                                        index=False, header=None)\n",
    "\n",
    "        return self.train_prediction_dict\n",
    "\n",
    "    def predict(self, test_data, lentest):\n",
    "        self.test_data = test_data\n",
    "        if self.task_type == 'classification':\n",
    "            test_prediction_shape = (lentest, self.num_classes)\n",
    "        else:\n",
    "            test_prediction_shape = (lentest, 1)\n",
    "\n",
    "        self.test_prediction_dict = {}\n",
    "        for level in range(self.levels):\n",
    "            self.test_prediction_dict[level] = np.zeros((test_prediction_shape[0],\n",
    "                                                         test_prediction_shape[1] * len(self.model_dict[level])))\n",
    "        self.test_data = test_data\n",
    "        for level in range(self.levels):\n",
    "            if level == 0:\n",
    "                temp_train = self.training_data\n",
    "                temp_test = self.test_data\n",
    "            else:\n",
    "                temp_train = self.train_prediction_dict[level - 1]\n",
    "                temp_test = self.test_prediction_dict[level - 1]\n",
    "\n",
    "            for model_num, model in enumerate(self.model_dict[level]):\n",
    "\n",
    "                logger.info(\"Training Fulldata Level %d. Model # %d\", level, model_num)\n",
    "                if level == 0:\n",
    "                    model.fit(temp_train[0][model_num], self.y_enc)\n",
    "                else:\n",
    "                    model.fit(temp_train, self.y_enc)\n",
    "\n",
    "                logger.info(\"Predicting Test Level %d. Model # %d\", level, model_num)\n",
    "\n",
    "                if self.task_type == 'classification':\n",
    "                    if level == 0:\n",
    "                        temp_test_predictions = model.predict_proba(temp_test[0][model_num])\n",
    "                    else:\n",
    "                        temp_test_predictions = model.predict_proba(temp_test)\n",
    "                    self.test_prediction_dict[level][:, (model_num * self.num_classes): (model_num * self.num_classes) +\n",
    "                                                                                        self.num_classes] = temp_test_predictions\n",
    "\n",
    "                else:\n",
    "                    if level == 0:\n",
    "                        temp_test_predictions = model.predict(temp_test[0][model_num])\n",
    "                    else:\n",
    "                        temp_test_predictions = model.predict(temp_test)\n",
    "                    self.test_prediction_dict[level][:, model_num] = temp_test_predictions\n",
    "\n",
    "            test_predictions_df = pd.DataFrame(self.test_prediction_dict[level])\n",
    "            test_predictions_df.to_csv(os.path.join(self.save_path, \"test_predictions_level_\" + str(level) + \".csv\"),\n",
    "                                       index=False, header=None)\n",
    "\n",
    "        return self.test_prediction_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "_cell_guid": "f5498ddf-1cf7-4d67-bdf4-e72d3c59b664",
    "_uuid": "94587d1b4de2770a35027d24c2184982b7b7a34a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:12:30] INFO Found 3 classes\n",
      "[19:12:30] INFO Training Level 0 Fold # 1. Model # 0\n",
      "[19:12:32] INFO Predicting Level 0. Fold # 1. Model # 0\n",
      "[19:12:32] INFO Level 0. Fold # 1. Model # 0. Validation Score = 0.626621\n",
      "[19:12:32] INFO Training Level 0 Fold # 2. Model # 0\n",
      "[19:12:34] INFO Predicting Level 0. Fold # 2. Model # 0\n",
      "[19:12:34] INFO Level 0. Fold # 2. Model # 0. Validation Score = 0.616463\n",
      "[19:12:34] INFO Training Level 0 Fold # 3. Model # 0\n",
      "[19:12:37] INFO Predicting Level 0. Fold # 3. Model # 0\n",
      "[19:12:37] INFO Level 0. Fold # 3. Model # 0. Validation Score = 0.619626\n",
      "[19:12:37] INFO Level 0. Model # 0. Mean Score = 0.620903. Std Dev = 0.004244\n",
      "[19:12:37] INFO Training Level 0 Fold # 1. Model # 1\n",
      "[19:12:57] INFO Predicting Level 0. Fold # 1. Model # 1\n",
      "[19:12:57] INFO Level 0. Fold # 1. Model # 1. Validation Score = 0.573485\n",
      "[19:12:57] INFO Training Level 0 Fold # 2. Model # 1\n",
      "[19:13:17] INFO Predicting Level 0. Fold # 2. Model # 1\n",
      "[19:13:17] INFO Level 0. Fold # 2. Model # 1. Validation Score = 0.563451\n",
      "[19:13:17] INFO Training Level 0 Fold # 3. Model # 1\n",
      "[19:13:39] INFO Predicting Level 0. Fold # 3. Model # 1\n",
      "[19:13:39] INFO Level 0. Fold # 3. Model # 1. Validation Score = 0.567765\n",
      "[19:13:39] INFO Level 0. Model # 1. Mean Score = 0.568233. Std Dev = 0.004110\n",
      "[19:13:39] INFO Training Level 0 Fold # 1. Model # 2\n",
      "[19:13:39] INFO Predicting Level 0. Fold # 1. Model # 2\n",
      "[19:13:39] INFO Level 0. Fold # 1. Model # 2. Validation Score = 0.463292\n",
      "[19:13:39] INFO Training Level 0 Fold # 2. Model # 2\n",
      "[19:13:39] INFO Predicting Level 0. Fold # 2. Model # 2\n",
      "[19:13:39] INFO Level 0. Fold # 2. Model # 2. Validation Score = 0.456477\n",
      "[19:13:39] INFO Training Level 0 Fold # 3. Model # 2\n",
      "[19:13:39] INFO Predicting Level 0. Fold # 3. Model # 2\n",
      "[19:13:39] INFO Level 0. Fold # 3. Model # 2. Validation Score = 0.461664\n",
      "[19:13:39] INFO Level 0. Model # 2. Mean Score = 0.460478. Std Dev = 0.002906\n",
      "[19:13:39] INFO Training Level 0 Fold # 1. Model # 3\n",
      "[19:13:39] INFO Predicting Level 0. Fold # 1. Model # 3\n",
      "[19:13:39] INFO Level 0. Fold # 1. Model # 3. Validation Score = 0.472378\n",
      "[19:13:39] INFO Training Level 0 Fold # 2. Model # 3\n",
      "[19:13:39] INFO Predicting Level 0. Fold # 2. Model # 3\n",
      "[19:13:39] INFO Level 0. Fold # 2. Model # 3. Validation Score = 0.473229\n",
      "[19:13:39] INFO Training Level 0 Fold # 3. Model # 3\n",
      "[19:13:39] INFO Predicting Level 0. Fold # 3. Model # 3\n",
      "[19:13:39] INFO Level 0. Fold # 3. Model # 3. Validation Score = 0.479033\n",
      "[19:13:39] INFO Level 0. Model # 3. Mean Score = 0.474880. Std Dev = 0.002957\n",
      "[19:13:39] INFO Saving predictions for level # 0\n",
      "[19:13:40] INFO Training Level 1 Fold # 1. Model # 0\n",
      "[19:13:46] INFO Predicting Level 1. Fold # 1. Model # 0\n",
      "[19:13:46] INFO Level 1. Fold # 1. Model # 0. Validation Score = 0.483467\n",
      "[19:13:46] INFO Training Level 1 Fold # 2. Model # 0\n",
      "[19:13:50] INFO Predicting Level 1. Fold # 2. Model # 0\n",
      "[19:13:50] INFO Level 1. Fold # 2. Model # 0. Validation Score = 0.475460\n",
      "[19:13:50] INFO Training Level 1 Fold # 3. Model # 0\n",
      "[19:13:57] INFO Predicting Level 1. Fold # 3. Model # 0\n",
      "[19:13:57] INFO Level 1. Fold # 3. Model # 0. Validation Score = 0.491676\n",
      "[19:13:57] INFO Level 1. Model # 0. Mean Score = 0.483534. Std Dev = 0.006620\n",
      "[19:13:57] INFO Saving predictions for level # 1\n",
      "[19:13:57] INFO Training Fulldata Level 0. Model # 0\n",
      "[19:13:59] INFO Predicting Test Level 0. Model # 0\n",
      "[19:13:59] INFO Training Fulldata Level 0. Model # 1\n",
      "[19:14:22] INFO Predicting Test Level 0. Model # 1\n",
      "[19:14:22] INFO Training Fulldata Level 0. Model # 2\n",
      "[19:14:22] INFO Predicting Test Level 0. Model # 2\n",
      "[19:14:22] INFO Training Fulldata Level 0. Model # 3\n",
      "[19:14:22] INFO Predicting Test Level 0. Model # 3\n",
      "[19:14:22] INFO Training Fulldata Level 1. Model # 0\n",
      "[19:14:29] INFO Predicting Test Level 1. Model # 0\n"
     ]
    }
   ],
   "source": [
    "# specify the data to be used for every level of ensembling:\n",
    "train_data_dict = {0: [xtrain_tfv, xtrain_ctv, xtrain_tfv, xtrain_ctv], 1: [xtrain_glove]}\n",
    "test_data_dict = {0: [xvalid_tfv, xvalid_ctv, xvalid_tfv, xvalid_ctv], 1: [xvalid_glove]}\n",
    "\n",
    "model_dict = {0: [LogisticRegression(), LogisticRegression(), MultinomialNB(alpha=0.1), MultinomialNB()],\n",
    "\n",
    "              1: [xgb.XGBClassifier(silent=True, n_estimators=120, max_depth=7)]}\n",
    "\n",
    "ens = Ensembler(model_dict=model_dict, num_folds=3, task_type='classification',\n",
    "                optimize=multiclass_logloss, lower_is_better=True, save_path='')\n",
    "\n",
    "ens.fit(train_data_dict, ytrain, lentrain=xtrain_glove.shape[0])\n",
    "preds = ens.predict(test_data_dict, lentest=xvalid_glove.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "_cell_guid": "85e52734-06b5-4025-a9a3-cf9697d66a45",
    "_uuid": "6a1962f3a313d10a6563eb0c8f35d8e7051c3f15"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4612325034912492"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check error:\n",
    "multiclass_logloss(yvalid, preds[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "161adbaf-9eec-4e13-87ee-24e4621ee6e0",
    "_uuid": "1ec49abaf6fce8fee5a59f8fc56c85d0ed7626d5"
   },
   "source": [
    "Thus, we see that ensembling improves the score by a great extent! Since this is supposed to be a tutorial only I wont be providing any CSVs that you can submit to the leaderboard."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
